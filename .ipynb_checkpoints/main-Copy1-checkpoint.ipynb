{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.mycnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 3920) loss: 2.354935\n",
      "(Epoch 0 / 40) train acc: 0.114000; val_acc: 0.104000\n",
      "(Iteration 6 / 3920) loss: 2.354672\n",
      "(Iteration 11 / 3920) loss: 2.354343\n",
      "(Iteration 16 / 3920) loss: 2.354293\n",
      "(Iteration 21 / 3920) loss: 2.353563\n",
      "(Iteration 26 / 3920) loss: 2.353147\n",
      "(Iteration 31 / 3920) loss: 2.352061\n",
      "(Iteration 36 / 3920) loss: 2.350967\n",
      "(Iteration 41 / 3920) loss: 2.349710\n",
      "(Iteration 46 / 3920) loss: 2.348189\n",
      "(Iteration 51 / 3920) loss: 2.345151\n",
      "(Iteration 56 / 3920) loss: 2.338760\n",
      "(Iteration 61 / 3920) loss: 2.336291\n",
      "(Iteration 66 / 3920) loss: 2.330859\n",
      "(Iteration 71 / 3920) loss: 2.320738\n",
      "(Iteration 76 / 3920) loss: 2.312269\n",
      "(Iteration 81 / 3920) loss: 2.301732\n",
      "(Iteration 86 / 3920) loss: 2.284299\n",
      "(Iteration 91 / 3920) loss: 2.254991\n",
      "(Iteration 96 / 3920) loss: 2.249212\n",
      "(Epoch 1 / 40) train acc: 0.194000; val_acc: 0.191000\n",
      "(Iteration 101 / 3920) loss: 2.232814\n",
      "(Iteration 106 / 3920) loss: 2.219571\n",
      "(Iteration 111 / 3920) loss: 2.184571\n",
      "(Iteration 116 / 3920) loss: 2.184784\n",
      "(Iteration 121 / 3920) loss: 2.125573\n",
      "(Iteration 126 / 3920) loss: 2.099000\n",
      "(Iteration 131 / 3920) loss: 2.100042\n",
      "(Iteration 136 / 3920) loss: 2.041367\n",
      "(Iteration 141 / 3920) loss: 2.032136\n",
      "(Iteration 146 / 3920) loss: 2.033601\n",
      "(Iteration 151 / 3920) loss: 1.997775\n",
      "(Iteration 156 / 3920) loss: 1.942805\n",
      "(Iteration 161 / 3920) loss: 1.961950\n",
      "(Iteration 166 / 3920) loss: 1.969617\n",
      "(Iteration 171 / 3920) loss: 1.931718\n",
      "(Iteration 176 / 3920) loss: 1.898958\n",
      "(Iteration 181 / 3920) loss: 1.856329\n",
      "(Iteration 186 / 3920) loss: 1.865257\n",
      "(Iteration 191 / 3920) loss: 1.864597\n",
      "(Iteration 196 / 3920) loss: 1.861478\n",
      "(Epoch 2 / 40) train acc: 0.301000; val_acc: 0.331000\n",
      "(Iteration 201 / 3920) loss: 1.860381\n",
      "(Iteration 206 / 3920) loss: 1.844464\n",
      "(Iteration 211 / 3920) loss: 1.845363\n",
      "(Iteration 216 / 3920) loss: 1.866784\n",
      "(Iteration 221 / 3920) loss: 1.817456\n",
      "(Iteration 226 / 3920) loss: 1.796567\n",
      "(Iteration 231 / 3920) loss: 1.801768\n",
      "(Iteration 236 / 3920) loss: 1.867588\n",
      "(Iteration 241 / 3920) loss: 1.814185\n",
      "(Iteration 246 / 3920) loss: 1.764883\n",
      "(Iteration 251 / 3920) loss: 1.805629\n",
      "(Iteration 256 / 3920) loss: 1.743163\n",
      "(Iteration 261 / 3920) loss: 1.782356\n",
      "(Iteration 266 / 3920) loss: 1.728675\n",
      "(Iteration 271 / 3920) loss: 1.690882\n",
      "(Iteration 276 / 3920) loss: 1.716202\n",
      "(Iteration 281 / 3920) loss: 1.776481\n",
      "(Iteration 286 / 3920) loss: 1.776479\n",
      "(Iteration 291 / 3920) loss: 1.716632\n",
      "(Epoch 3 / 40) train acc: 0.389000; val_acc: 0.389000\n",
      "(Iteration 296 / 3920) loss: 1.703083\n",
      "(Iteration 301 / 3920) loss: 1.693756\n",
      "(Iteration 306 / 3920) loss: 1.747587\n",
      "(Iteration 311 / 3920) loss: 1.717429\n",
      "(Iteration 316 / 3920) loss: 1.755096\n",
      "(Iteration 321 / 3920) loss: 1.683299\n",
      "(Iteration 326 / 3920) loss: 1.693915\n",
      "(Iteration 331 / 3920) loss: 1.621972\n",
      "(Iteration 336 / 3920) loss: 1.692367\n",
      "(Iteration 341 / 3920) loss: 1.670486\n",
      "(Iteration 346 / 3920) loss: 1.727361\n",
      "(Iteration 351 / 3920) loss: 1.666754\n",
      "(Iteration 356 / 3920) loss: 1.671349\n",
      "(Iteration 361 / 3920) loss: 1.622850\n",
      "(Iteration 366 / 3920) loss: 1.641042\n",
      "(Iteration 371 / 3920) loss: 1.712212\n",
      "(Iteration 376 / 3920) loss: 1.662657\n",
      "(Iteration 381 / 3920) loss: 1.676328\n",
      "(Iteration 386 / 3920) loss: 1.610629\n",
      "(Iteration 391 / 3920) loss: 1.648245\n",
      "(Epoch 4 / 40) train acc: 0.447000; val_acc: 0.425000\n",
      "(Iteration 396 / 3920) loss: 1.619603\n",
      "(Iteration 401 / 3920) loss: 1.619633\n",
      "(Iteration 406 / 3920) loss: 1.587066\n",
      "(Iteration 411 / 3920) loss: 1.644334\n",
      "(Iteration 416 / 3920) loss: 1.586853\n",
      "(Iteration 421 / 3920) loss: 1.633519\n",
      "(Iteration 426 / 3920) loss: 1.646812\n",
      "(Iteration 431 / 3920) loss: 1.597227\n",
      "(Iteration 436 / 3920) loss: 1.568016\n",
      "(Iteration 441 / 3920) loss: 1.621543\n",
      "(Iteration 446 / 3920) loss: 1.645306\n",
      "(Iteration 451 / 3920) loss: 1.652678\n",
      "(Iteration 456 / 3920) loss: 1.618565\n",
      "(Iteration 461 / 3920) loss: 1.616452\n",
      "(Iteration 466 / 3920) loss: 1.550839\n",
      "(Iteration 471 / 3920) loss: 1.513994\n",
      "(Iteration 476 / 3920) loss: 1.601010\n",
      "(Iteration 481 / 3920) loss: 1.614583\n",
      "(Iteration 486 / 3920) loss: 1.591137\n",
      "(Epoch 5 / 40) train acc: 0.452000; val_acc: 0.452000\n",
      "(Iteration 491 / 3920) loss: 1.511082\n",
      "(Iteration 496 / 3920) loss: 1.572171\n",
      "(Iteration 501 / 3920) loss: 1.557804\n",
      "(Iteration 506 / 3920) loss: 1.578073\n",
      "(Iteration 511 / 3920) loss: 1.547082\n",
      "(Iteration 516 / 3920) loss: 1.502805\n",
      "(Iteration 521 / 3920) loss: 1.522837\n",
      "(Iteration 526 / 3920) loss: 1.510791\n",
      "(Iteration 531 / 3920) loss: 1.558090\n",
      "(Iteration 536 / 3920) loss: 1.533736\n",
      "(Iteration 541 / 3920) loss: 1.526640\n",
      "(Iteration 546 / 3920) loss: 1.493842\n",
      "(Iteration 551 / 3920) loss: 1.492306\n",
      "(Iteration 556 / 3920) loss: 1.549220\n",
      "(Iteration 561 / 3920) loss: 1.499061\n",
      "(Iteration 566 / 3920) loss: 1.558372\n",
      "(Iteration 571 / 3920) loss: 1.564682\n",
      "(Iteration 576 / 3920) loss: 1.480344\n",
      "(Iteration 581 / 3920) loss: 1.531151\n",
      "(Iteration 586 / 3920) loss: 1.548827\n",
      "(Epoch 6 / 40) train acc: 0.499000; val_acc: 0.462000\n",
      "(Iteration 591 / 3920) loss: 1.496896\n",
      "(Iteration 596 / 3920) loss: 1.548950\n",
      "(Iteration 601 / 3920) loss: 1.484347\n",
      "(Iteration 606 / 3920) loss: 1.565049\n",
      "(Iteration 611 / 3920) loss: 1.468934\n",
      "(Iteration 616 / 3920) loss: 1.481660\n",
      "(Iteration 621 / 3920) loss: 1.475061\n",
      "(Iteration 626 / 3920) loss: 1.518165\n",
      "(Iteration 631 / 3920) loss: 1.525463\n",
      "(Iteration 636 / 3920) loss: 1.453823\n",
      "(Iteration 641 / 3920) loss: 1.468776\n",
      "(Iteration 646 / 3920) loss: 1.528980\n",
      "(Iteration 651 / 3920) loss: 1.393069\n",
      "(Iteration 656 / 3920) loss: 1.539802\n",
      "(Iteration 661 / 3920) loss: 1.469362\n",
      "(Iteration 666 / 3920) loss: 1.467098\n",
      "(Iteration 671 / 3920) loss: 1.575454\n",
      "(Iteration 676 / 3920) loss: 1.483232\n",
      "(Iteration 681 / 3920) loss: 1.491184\n",
      "(Iteration 686 / 3920) loss: 1.454649\n",
      "(Epoch 7 / 40) train acc: 0.491000; val_acc: 0.492000\n",
      "(Iteration 691 / 3920) loss: 1.390769\n",
      "(Iteration 696 / 3920) loss: 1.449025\n",
      "(Iteration 701 / 3920) loss: 1.439099\n",
      "(Iteration 706 / 3920) loss: 1.390162\n",
      "(Iteration 711 / 3920) loss: 1.449077\n",
      "(Iteration 716 / 3920) loss: 1.422268\n",
      "(Iteration 721 / 3920) loss: 1.488422\n",
      "(Iteration 726 / 3920) loss: 1.430133\n",
      "(Iteration 731 / 3920) loss: 1.434036\n",
      "(Iteration 736 / 3920) loss: 1.435866\n",
      "(Iteration 741 / 3920) loss: 1.417068\n",
      "(Iteration 746 / 3920) loss: 1.418075\n",
      "(Iteration 751 / 3920) loss: 1.410573\n",
      "(Iteration 756 / 3920) loss: 1.376910\n",
      "(Iteration 761 / 3920) loss: 1.425955\n",
      "(Iteration 766 / 3920) loss: 1.414970\n",
      "(Iteration 771 / 3920) loss: 1.377500\n",
      "(Iteration 776 / 3920) loss: 1.412865\n",
      "(Iteration 781 / 3920) loss: 1.431027\n",
      "(Epoch 8 / 40) train acc: 0.515000; val_acc: 0.515000\n",
      "(Iteration 786 / 3920) loss: 1.409558\n",
      "(Iteration 791 / 3920) loss: 1.342212\n",
      "(Iteration 796 / 3920) loss: 1.343778\n",
      "(Iteration 801 / 3920) loss: 1.416328\n",
      "(Iteration 806 / 3920) loss: 1.475730\n",
      "(Iteration 811 / 3920) loss: 1.389683\n",
      "(Iteration 816 / 3920) loss: 1.442716\n",
      "(Iteration 821 / 3920) loss: 1.387105\n",
      "(Iteration 826 / 3920) loss: 1.351088\n",
      "(Iteration 831 / 3920) loss: 1.399132\n",
      "(Iteration 836 / 3920) loss: 1.416813\n",
      "(Iteration 841 / 3920) loss: 1.408115\n",
      "(Iteration 846 / 3920) loss: 1.382509\n",
      "(Iteration 851 / 3920) loss: 1.384137\n",
      "(Iteration 856 / 3920) loss: 1.406372\n",
      "(Iteration 861 / 3920) loss: 1.326595\n",
      "(Iteration 866 / 3920) loss: 1.430462\n",
      "(Iteration 871 / 3920) loss: 1.361472\n",
      "(Iteration 876 / 3920) loss: 1.309669\n",
      "(Iteration 881 / 3920) loss: 1.462079\n",
      "(Epoch 9 / 40) train acc: 0.535000; val_acc: 0.521000\n",
      "(Iteration 886 / 3920) loss: 1.362262\n",
      "(Iteration 891 / 3920) loss: 1.326924\n",
      "(Iteration 896 / 3920) loss: 1.350783\n",
      "(Iteration 901 / 3920) loss: 1.383911\n",
      "(Iteration 906 / 3920) loss: 1.334285\n",
      "(Iteration 911 / 3920) loss: 1.357992\n",
      "(Iteration 916 / 3920) loss: 1.352340\n",
      "(Iteration 921 / 3920) loss: 1.441757\n",
      "(Iteration 926 / 3920) loss: 1.304147\n",
      "(Iteration 931 / 3920) loss: 1.383512\n",
      "(Iteration 936 / 3920) loss: 1.351799\n",
      "(Iteration 941 / 3920) loss: 1.365887\n",
      "(Iteration 946 / 3920) loss: 1.336055\n",
      "(Iteration 951 / 3920) loss: 1.447250\n",
      "(Iteration 956 / 3920) loss: 1.240828\n",
      "(Iteration 961 / 3920) loss: 1.368795\n",
      "(Iteration 966 / 3920) loss: 1.344395\n",
      "(Iteration 971 / 3920) loss: 1.298600\n",
      "(Iteration 976 / 3920) loss: 1.287056\n",
      "(Epoch 10 / 40) train acc: 0.519000; val_acc: 0.537000\n",
      "(Iteration 981 / 3920) loss: 1.319864\n",
      "(Iteration 986 / 3920) loss: 1.247055\n",
      "(Iteration 991 / 3920) loss: 1.264175\n",
      "(Iteration 996 / 3920) loss: 1.339907\n",
      "(Iteration 1001 / 3920) loss: 1.358512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1006 / 3920) loss: 1.331728\n",
      "(Iteration 1011 / 3920) loss: 1.249947\n",
      "(Iteration 1016 / 3920) loss: 1.336094\n",
      "(Iteration 1021 / 3920) loss: 1.332191\n",
      "(Iteration 1026 / 3920) loss: 1.312084\n",
      "(Iteration 1031 / 3920) loss: 1.291607\n",
      "(Iteration 1036 / 3920) loss: 1.343749\n",
      "(Iteration 1041 / 3920) loss: 1.335780\n",
      "(Iteration 1046 / 3920) loss: 1.299294\n",
      "(Iteration 1051 / 3920) loss: 1.275497\n",
      "(Iteration 1056 / 3920) loss: 1.320817\n",
      "(Iteration 1061 / 3920) loss: 1.379647\n",
      "(Iteration 1066 / 3920) loss: 1.198491\n",
      "(Iteration 1071 / 3920) loss: 1.285437\n",
      "(Iteration 1076 / 3920) loss: 1.351334\n",
      "(Epoch 11 / 40) train acc: 0.537000; val_acc: 0.541000\n",
      "(Iteration 1081 / 3920) loss: 1.373149\n",
      "(Iteration 1086 / 3920) loss: 1.271908\n",
      "(Iteration 1091 / 3920) loss: 1.301149\n",
      "(Iteration 1096 / 3920) loss: 1.312738\n",
      "(Iteration 1101 / 3920) loss: 1.271958\n",
      "(Iteration 1106 / 3920) loss: 1.350178\n",
      "(Iteration 1111 / 3920) loss: 1.295059\n",
      "(Iteration 1116 / 3920) loss: 1.322899\n",
      "(Iteration 1121 / 3920) loss: 1.233689\n",
      "(Iteration 1126 / 3920) loss: 1.286621\n",
      "(Iteration 1131 / 3920) loss: 1.322146\n",
      "(Iteration 1136 / 3920) loss: 1.230013\n",
      "(Iteration 1141 / 3920) loss: 1.281722\n",
      "(Iteration 1146 / 3920) loss: 1.332451\n",
      "(Iteration 1151 / 3920) loss: 1.294563\n",
      "(Iteration 1156 / 3920) loss: 1.370472\n",
      "(Iteration 1161 / 3920) loss: 1.276209\n",
      "(Iteration 1166 / 3920) loss: 1.303303\n",
      "(Iteration 1171 / 3920) loss: 1.338689\n",
      "(Iteration 1176 / 3920) loss: 1.316540\n",
      "(Epoch 12 / 40) train acc: 0.562000; val_acc: 0.550000\n",
      "(Iteration 1181 / 3920) loss: 1.197695\n",
      "(Iteration 1186 / 3920) loss: 1.323796\n",
      "(Iteration 1191 / 3920) loss: 1.356401\n",
      "(Iteration 1196 / 3920) loss: 1.295995\n",
      "(Iteration 1201 / 3920) loss: 1.302673\n",
      "(Iteration 1206 / 3920) loss: 1.229759\n",
      "(Iteration 1211 / 3920) loss: 1.291992\n",
      "(Iteration 1216 / 3920) loss: 1.278710\n",
      "(Iteration 1221 / 3920) loss: 1.239673\n",
      "(Iteration 1226 / 3920) loss: 1.299584\n",
      "(Iteration 1231 / 3920) loss: 1.369588\n",
      "(Iteration 1236 / 3920) loss: 1.305936\n",
      "(Iteration 1241 / 3920) loss: 1.247403\n",
      "(Iteration 1246 / 3920) loss: 1.266058\n",
      "(Iteration 1251 / 3920) loss: 1.294491\n",
      "(Iteration 1256 / 3920) loss: 1.260645\n",
      "(Iteration 1261 / 3920) loss: 1.269631\n",
      "(Iteration 1266 / 3920) loss: 1.311166\n",
      "(Iteration 1271 / 3920) loss: 1.366440\n",
      "(Epoch 13 / 40) train acc: 0.565000; val_acc: 0.560000\n",
      "(Iteration 1276 / 3920) loss: 1.278345\n",
      "(Iteration 1281 / 3920) loss: 1.220460\n",
      "(Iteration 1286 / 3920) loss: 1.306945\n",
      "(Iteration 1291 / 3920) loss: 1.263748\n",
      "(Iteration 1296 / 3920) loss: 1.332905\n",
      "(Iteration 1301 / 3920) loss: 1.309020\n",
      "(Iteration 1306 / 3920) loss: 1.266797\n",
      "(Iteration 1311 / 3920) loss: 1.281413\n",
      "(Iteration 1316 / 3920) loss: 1.250742\n",
      "(Iteration 1321 / 3920) loss: 1.183638\n",
      "(Iteration 1326 / 3920) loss: 1.205626\n",
      "(Iteration 1331 / 3920) loss: 1.327122\n",
      "(Iteration 1336 / 3920) loss: 1.254782\n",
      "(Iteration 1341 / 3920) loss: 1.170524\n",
      "(Iteration 1346 / 3920) loss: 1.212932\n",
      "(Iteration 1351 / 3920) loss: 1.237328\n",
      "(Iteration 1356 / 3920) loss: 1.251995\n",
      "(Iteration 1361 / 3920) loss: 1.210207\n",
      "(Iteration 1366 / 3920) loss: 1.197672\n",
      "(Iteration 1371 / 3920) loss: 1.218283\n",
      "(Epoch 14 / 40) train acc: 0.573000; val_acc: 0.565000\n",
      "(Iteration 1376 / 3920) loss: 1.236687\n",
      "(Iteration 1381 / 3920) loss: 1.201775\n",
      "(Iteration 1386 / 3920) loss: 1.263215\n",
      "(Iteration 1391 / 3920) loss: 1.217629\n",
      "(Iteration 1396 / 3920) loss: 1.200333\n",
      "(Iteration 1401 / 3920) loss: 1.173252\n",
      "(Iteration 1406 / 3920) loss: 1.213047\n",
      "(Iteration 1411 / 3920) loss: 1.183021\n",
      "(Iteration 1416 / 3920) loss: 1.131504\n",
      "(Iteration 1421 / 3920) loss: 1.183721\n",
      "(Iteration 1426 / 3920) loss: 1.259251\n",
      "(Iteration 1431 / 3920) loss: 1.251215\n",
      "(Iteration 1436 / 3920) loss: 1.254053\n",
      "(Iteration 1441 / 3920) loss: 1.175790\n",
      "(Iteration 1446 / 3920) loss: 1.216095\n",
      "(Iteration 1451 / 3920) loss: 1.241985\n",
      "(Iteration 1456 / 3920) loss: 1.253411\n",
      "(Iteration 1461 / 3920) loss: 1.315631\n",
      "(Iteration 1466 / 3920) loss: 1.305247\n",
      "(Epoch 15 / 40) train acc: 0.566000; val_acc: 0.576000\n",
      "(Iteration 1471 / 3920) loss: 1.149745\n",
      "(Iteration 1476 / 3920) loss: 1.261738\n",
      "(Iteration 1481 / 3920) loss: 1.183302\n",
      "(Iteration 1486 / 3920) loss: 1.236221\n",
      "(Iteration 1491 / 3920) loss: 1.203889\n",
      "(Iteration 1496 / 3920) loss: 1.221441\n",
      "(Iteration 1501 / 3920) loss: 1.233170\n",
      "(Iteration 1506 / 3920) loss: 1.236847\n",
      "(Iteration 1511 / 3920) loss: 1.241472\n",
      "(Iteration 1516 / 3920) loss: 1.212825\n",
      "(Iteration 1521 / 3920) loss: 1.290501\n",
      "(Iteration 1526 / 3920) loss: 1.164507\n",
      "(Iteration 1531 / 3920) loss: 1.218009\n",
      "(Iteration 1536 / 3920) loss: 1.227087\n",
      "(Iteration 1541 / 3920) loss: 1.191783\n",
      "(Iteration 1546 / 3920) loss: 1.163965\n",
      "(Iteration 1551 / 3920) loss: 1.171667\n",
      "(Iteration 1556 / 3920) loss: 1.183262\n",
      "(Iteration 1561 / 3920) loss: 1.171284\n",
      "(Iteration 1566 / 3920) loss: 1.237033\n",
      "(Epoch 16 / 40) train acc: 0.592000; val_acc: 0.591000\n",
      "(Iteration 1571 / 3920) loss: 1.210079\n",
      "(Iteration 1576 / 3920) loss: 1.230076\n",
      "(Iteration 1581 / 3920) loss: 1.107869\n",
      "(Iteration 1586 / 3920) loss: 1.139188\n",
      "(Iteration 1591 / 3920) loss: 1.175026\n",
      "(Iteration 1596 / 3920) loss: 1.231715\n",
      "(Iteration 1601 / 3920) loss: 1.198038\n",
      "(Iteration 1606 / 3920) loss: 1.230549\n",
      "(Iteration 1611 / 3920) loss: 1.184693\n",
      "(Iteration 1616 / 3920) loss: 1.091624\n",
      "(Iteration 1621 / 3920) loss: 1.191140\n",
      "(Iteration 1626 / 3920) loss: 1.235397\n",
      "(Iteration 1631 / 3920) loss: 1.193900\n",
      "(Iteration 1636 / 3920) loss: 1.188881\n",
      "(Iteration 1641 / 3920) loss: 1.142992\n",
      "(Iteration 1646 / 3920) loss: 1.154384\n",
      "(Iteration 1651 / 3920) loss: 1.249588\n",
      "(Iteration 1656 / 3920) loss: 1.178780\n",
      "(Iteration 1661 / 3920) loss: 1.228792\n",
      "(Iteration 1666 / 3920) loss: 1.153898\n",
      "(Epoch 17 / 40) train acc: 0.605000; val_acc: 0.597000\n",
      "(Iteration 1671 / 3920) loss: 1.162705\n",
      "(Iteration 1676 / 3920) loss: 1.161639\n",
      "(Iteration 1681 / 3920) loss: 1.200796\n",
      "(Iteration 1686 / 3920) loss: 1.207686\n",
      "(Iteration 1691 / 3920) loss: 1.193767\n",
      "(Iteration 1696 / 3920) loss: 1.154864\n",
      "(Iteration 1701 / 3920) loss: 1.131031\n",
      "(Iteration 1706 / 3920) loss: 1.221993\n",
      "(Iteration 1711 / 3920) loss: 1.161348\n",
      "(Iteration 1716 / 3920) loss: 1.115958\n",
      "(Iteration 1721 / 3920) loss: 1.148396\n",
      "(Iteration 1726 / 3920) loss: 1.197797\n",
      "(Iteration 1731 / 3920) loss: 1.187363\n",
      "(Iteration 1736 / 3920) loss: 1.170649\n",
      "(Iteration 1741 / 3920) loss: 1.250017\n",
      "(Iteration 1746 / 3920) loss: 1.078680\n",
      "(Iteration 1751 / 3920) loss: 1.174106\n",
      "(Iteration 1756 / 3920) loss: 1.167928\n",
      "(Iteration 1761 / 3920) loss: 1.145838\n",
      "(Epoch 18 / 40) train acc: 0.603000; val_acc: 0.591000\n",
      "(Iteration 1766 / 3920) loss: 1.110228\n",
      "(Iteration 1771 / 3920) loss: 1.191400\n",
      "(Iteration 1776 / 3920) loss: 1.208277\n",
      "(Iteration 1781 / 3920) loss: 1.149753\n",
      "(Iteration 1786 / 3920) loss: 1.228244\n",
      "(Iteration 1791 / 3920) loss: 1.022216\n",
      "(Iteration 1796 / 3920) loss: 1.122153\n",
      "(Iteration 1801 / 3920) loss: 1.123536\n",
      "(Iteration 1806 / 3920) loss: 1.110840\n",
      "(Iteration 1811 / 3920) loss: 1.223134\n",
      "(Iteration 1816 / 3920) loss: 1.104002\n",
      "(Iteration 1821 / 3920) loss: 1.234148\n",
      "(Iteration 1826 / 3920) loss: 1.207234\n",
      "(Iteration 1831 / 3920) loss: 1.211428\n",
      "(Iteration 1836 / 3920) loss: 1.202245\n",
      "(Iteration 1841 / 3920) loss: 1.206261\n",
      "(Iteration 1846 / 3920) loss: 1.175337\n",
      "(Iteration 1851 / 3920) loss: 1.101781\n",
      "(Iteration 1856 / 3920) loss: 1.090610\n",
      "(Iteration 1861 / 3920) loss: 1.140743\n",
      "(Epoch 19 / 40) train acc: 0.621000; val_acc: 0.600000\n",
      "(Iteration 1866 / 3920) loss: 1.176909\n",
      "(Iteration 1871 / 3920) loss: 1.109981\n",
      "(Iteration 1876 / 3920) loss: 1.084461\n",
      "(Iteration 1881 / 3920) loss: 1.133422\n",
      "(Iteration 1886 / 3920) loss: 1.124119\n",
      "(Iteration 1891 / 3920) loss: 1.108417\n",
      "(Iteration 1896 / 3920) loss: 1.206407\n",
      "(Iteration 1901 / 3920) loss: 1.186044\n",
      "(Iteration 1906 / 3920) loss: 1.148463\n",
      "(Iteration 1911 / 3920) loss: 1.023589\n",
      "(Iteration 1916 / 3920) loss: 1.174613\n",
      "(Iteration 1921 / 3920) loss: 1.155233\n",
      "(Iteration 1926 / 3920) loss: 1.130509\n",
      "(Iteration 1931 / 3920) loss: 1.137485\n",
      "(Iteration 1936 / 3920) loss: 1.201748\n",
      "(Iteration 1941 / 3920) loss: 1.068597\n",
      "(Iteration 1946 / 3920) loss: 1.090351\n",
      "(Iteration 1951 / 3920) loss: 1.119161\n",
      "(Iteration 1956 / 3920) loss: 1.129316\n",
      "(Epoch 20 / 40) train acc: 0.616000; val_acc: 0.598000\n",
      "(Iteration 1961 / 3920) loss: 1.212845\n",
      "(Iteration 1966 / 3920) loss: 1.156051\n",
      "(Iteration 1971 / 3920) loss: 1.211544\n",
      "(Iteration 1976 / 3920) loss: 1.153072\n",
      "(Iteration 1981 / 3920) loss: 1.173112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1986 / 3920) loss: 1.142989\n",
      "(Iteration 1991 / 3920) loss: 1.191392\n",
      "(Iteration 1996 / 3920) loss: 1.121665\n",
      "(Iteration 2001 / 3920) loss: 1.183892\n",
      "(Iteration 2006 / 3920) loss: 1.153731\n",
      "(Iteration 2011 / 3920) loss: 1.089665\n",
      "(Iteration 2016 / 3920) loss: 1.102488\n",
      "(Iteration 2021 / 3920) loss: 1.138059\n",
      "(Iteration 2026 / 3920) loss: 1.150045\n",
      "(Iteration 2031 / 3920) loss: 1.034483\n",
      "(Iteration 2036 / 3920) loss: 1.057978\n",
      "(Iteration 2041 / 3920) loss: 1.281984\n",
      "(Iteration 2046 / 3920) loss: 1.124946\n",
      "(Iteration 2051 / 3920) loss: 1.103701\n",
      "(Iteration 2056 / 3920) loss: 1.138128\n",
      "(Epoch 21 / 40) train acc: 0.611000; val_acc: 0.602000\n",
      "(Iteration 2061 / 3920) loss: 1.128875\n",
      "(Iteration 2066 / 3920) loss: 1.186570\n",
      "(Iteration 2071 / 3920) loss: 1.139127\n",
      "(Iteration 2076 / 3920) loss: 1.110258\n",
      "(Iteration 2081 / 3920) loss: 1.067249\n",
      "(Iteration 2086 / 3920) loss: 1.126139\n",
      "(Iteration 2091 / 3920) loss: 1.128173\n",
      "(Iteration 2096 / 3920) loss: 1.155481\n",
      "(Iteration 2101 / 3920) loss: 1.127548\n",
      "(Iteration 2106 / 3920) loss: 1.132633\n",
      "(Iteration 2111 / 3920) loss: 1.064987\n",
      "(Iteration 2116 / 3920) loss: 1.108956\n",
      "(Iteration 2121 / 3920) loss: 1.143318\n",
      "(Iteration 2126 / 3920) loss: 1.089794\n",
      "(Iteration 2131 / 3920) loss: 1.124535\n",
      "(Iteration 2136 / 3920) loss: 1.121010\n",
      "(Iteration 2141 / 3920) loss: 1.064993\n",
      "(Iteration 2146 / 3920) loss: 1.072057\n",
      "(Iteration 2151 / 3920) loss: 1.129720\n",
      "(Iteration 2156 / 3920) loss: 1.095996\n",
      "(Epoch 22 / 40) train acc: 0.607000; val_acc: 0.601000\n",
      "(Iteration 2161 / 3920) loss: 1.174312\n",
      "(Iteration 2166 / 3920) loss: 1.117483\n",
      "(Iteration 2171 / 3920) loss: 1.195995\n",
      "(Iteration 2176 / 3920) loss: 1.127742\n",
      "(Iteration 2181 / 3920) loss: 1.069083\n",
      "(Iteration 2186 / 3920) loss: 1.161162\n",
      "(Iteration 2191 / 3920) loss: 1.112248\n",
      "(Iteration 2196 / 3920) loss: 1.142400\n",
      "(Iteration 2201 / 3920) loss: 1.089635\n",
      "(Iteration 2206 / 3920) loss: 1.159852\n",
      "(Iteration 2211 / 3920) loss: 1.100542\n",
      "(Iteration 2216 / 3920) loss: 1.033150\n",
      "(Iteration 2221 / 3920) loss: 1.026449\n",
      "(Iteration 2226 / 3920) loss: 1.130376\n",
      "(Iteration 2231 / 3920) loss: 1.068969\n",
      "(Iteration 2236 / 3920) loss: 1.069419\n",
      "(Iteration 2241 / 3920) loss: 1.130971\n",
      "(Iteration 2246 / 3920) loss: 1.100644\n",
      "(Iteration 2251 / 3920) loss: 1.190852\n",
      "(Epoch 23 / 40) train acc: 0.628000; val_acc: 0.612000\n",
      "(Iteration 2256 / 3920) loss: 1.038326\n",
      "(Iteration 2261 / 3920) loss: 1.043153\n",
      "(Iteration 2266 / 3920) loss: 1.087798\n",
      "(Iteration 2271 / 3920) loss: 1.109549\n",
      "(Iteration 2276 / 3920) loss: 1.110524\n",
      "(Iteration 2281 / 3920) loss: 1.061177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9aebd05c89e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#                 verbose=True, print_every=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml2018/assignment2/cs231n/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# Maybe print training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml2018/assignment2/cs231n/solver.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Compute loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml2018/assignment2/cs231n/classifiers/mycnn.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mdup\u001b[0m   \u001b[0;34m=\u001b[0m  \u001b[0mmax_pool_backward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cachemaxPool2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gamma2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"beta2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_bn_relu_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cacheconv2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mdup\u001b[0m   \u001b[0;34m=\u001b[0m  \u001b[0mmax_pool_backward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cachemaxPool1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mdup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gamma1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"beta1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_bn_relu_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cacheconv1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml2018/assignment2/cs231n/fast_layers.py\u001b[0m in \u001b[0;36mmax_pool_backward_fast\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax_pool_backward_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'im2col'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax_pool_backward_im2col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml2018/assignment2/cs231n/fast_layers.py\u001b[0m in \u001b[0;36mmax_pool_backward_reshape\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mdout_newaxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mdout_broadcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout_newaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdx_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mdx_reshaped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdout_broadcast\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0mdx_reshaped\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdx_reshaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# batch normalization and dropout useful. Store your best model in the         #\n",
    "# best_model variable.                                                         #\n",
    "################################################################################\n",
    "model = MyCNN([512, 256, 256, 128, 128,64], weight_scale=4e-3,reg = 0.0002)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                print_every = 20,\n",
    "                  num_epochs=40, batch_size=64,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 3e-4\n",
    "                  },\n",
    "                  verbose=True)\n",
    "# num_train = 1000\n",
    "# small_data = {\n",
    "#   'X_train': data['X_train'][:num_train],\n",
    "#   'y_train': data['y_train'][:num_train],\n",
    "#   'X_val': data['X_val'],\n",
    "#   'y_val': data['y_val'],\n",
    "# }\n",
    "\n",
    "\n",
    "# solver = Solver(model, small_data,\n",
    "#                 num_epochs=100, batch_size=50,\n",
    "#                 update_rule='adam',\n",
    "#                 optim_config={\n",
    "#                   'learning_rate': 1e-3,\n",
    "#                 },\n",
    "#                 verbose=True, print_every=1)\n",
    "\n",
    "solver.train()\n",
    "solver.train()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(solver.loss_history)\n",
    "plt.title(\"loss history\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(solver.train_acc_history,label = 'train')\n",
    "plt.plot(solver.val_acc_history,label = 'val')\n",
    "plt.title(\"accuracy history\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "best_model = model\n",
    "##############\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
