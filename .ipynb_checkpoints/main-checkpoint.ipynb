{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:  (49000,)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.mycnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 1960) loss: 2.389605\n",
      "(Epoch 0 / 40) train acc: 0.086000; val_acc: 0.101000\n",
      "(Iteration 6 / 1960) loss: 2.389268\n",
      "(Iteration 11 / 1960) loss: 2.388881\n",
      "(Iteration 16 / 1960) loss: 2.388457\n",
      "(Iteration 21 / 1960) loss: 2.387883\n",
      "(Iteration 26 / 1960) loss: 2.387335\n",
      "(Iteration 31 / 1960) loss: 2.386623\n",
      "(Iteration 36 / 1960) loss: 2.385231\n",
      "(Iteration 41 / 1960) loss: 2.384145\n",
      "(Iteration 46 / 1960) loss: 2.381568\n",
      "(Epoch 1 / 40) train acc: 0.167000; val_acc: 0.152000\n",
      "(Iteration 51 / 1960) loss: 2.379643\n",
      "(Iteration 56 / 1960) loss: 2.375920\n",
      "(Iteration 61 / 1960) loss: 2.369209\n",
      "(Iteration 66 / 1960) loss: 2.361618\n",
      "(Iteration 71 / 1960) loss: 2.347559\n",
      "(Iteration 76 / 1960) loss: 2.338216\n",
      "(Iteration 81 / 1960) loss: 2.313203\n",
      "(Iteration 86 / 1960) loss: 2.283089\n",
      "(Iteration 91 / 1960) loss: 2.255978\n",
      "(Iteration 96 / 1960) loss: 2.214203\n",
      "(Epoch 2 / 40) train acc: 0.211000; val_acc: 0.264000\n",
      "(Iteration 101 / 1960) loss: 2.179987\n",
      "(Iteration 106 / 1960) loss: 2.137811\n",
      "(Iteration 111 / 1960) loss: 2.109974\n",
      "(Iteration 116 / 1960) loss: 2.054519\n",
      "(Iteration 121 / 1960) loss: 1.999348\n",
      "(Iteration 126 / 1960) loss: 1.966685\n",
      "(Iteration 131 / 1960) loss: 1.952118\n",
      "(Iteration 136 / 1960) loss: 1.907511\n",
      "(Iteration 141 / 1960) loss: 1.854469\n",
      "(Iteration 146 / 1960) loss: 1.870340\n",
      "(Epoch 3 / 40) train acc: 0.369000; val_acc: 0.377000\n",
      "(Iteration 151 / 1960) loss: 1.833456\n",
      "(Iteration 156 / 1960) loss: 1.854706\n",
      "(Iteration 161 / 1960) loss: 1.798422\n",
      "(Iteration 166 / 1960) loss: 1.772567\n",
      "(Iteration 171 / 1960) loss: 1.763098\n",
      "(Iteration 176 / 1960) loss: 1.740502\n",
      "(Iteration 181 / 1960) loss: 1.766581\n",
      "(Iteration 186 / 1960) loss: 1.739247\n",
      "(Iteration 191 / 1960) loss: 1.771928\n",
      "(Iteration 196 / 1960) loss: 1.702157\n",
      "(Epoch 4 / 40) train acc: 0.386000; val_acc: 0.406000\n",
      "(Iteration 201 / 1960) loss: 1.686403\n",
      "(Iteration 206 / 1960) loss: 1.679437\n",
      "(Iteration 211 / 1960) loss: 1.677756\n",
      "(Iteration 216 / 1960) loss: 1.665325\n",
      "(Iteration 221 / 1960) loss: 1.668331\n",
      "(Iteration 226 / 1960) loss: 1.632288\n",
      "(Iteration 231 / 1960) loss: 1.645991\n",
      "(Iteration 236 / 1960) loss: 1.638247\n",
      "(Iteration 241 / 1960) loss: 1.650157\n",
      "(Epoch 5 / 40) train acc: 0.471000; val_acc: 0.442000\n",
      "(Iteration 246 / 1960) loss: 1.663695\n",
      "(Iteration 251 / 1960) loss: 1.615867\n",
      "(Iteration 256 / 1960) loss: 1.662522\n",
      "(Iteration 261 / 1960) loss: 1.585967\n",
      "(Iteration 266 / 1960) loss: 1.564926\n",
      "(Iteration 271 / 1960) loss: 1.578003\n",
      "(Iteration 276 / 1960) loss: 1.582930\n",
      "(Iteration 281 / 1960) loss: 1.584277\n",
      "(Iteration 286 / 1960) loss: 1.578477\n",
      "(Iteration 291 / 1960) loss: 1.511896\n",
      "(Epoch 6 / 40) train acc: 0.469000; val_acc: 0.463000\n",
      "(Iteration 296 / 1960) loss: 1.559322\n",
      "(Iteration 301 / 1960) loss: 1.513799\n",
      "(Iteration 306 / 1960) loss: 1.547974\n",
      "(Iteration 311 / 1960) loss: 1.494776\n",
      "(Iteration 316 / 1960) loss: 1.497131\n",
      "(Iteration 321 / 1960) loss: 1.510717\n",
      "(Iteration 326 / 1960) loss: 1.518680\n",
      "(Iteration 331 / 1960) loss: 1.442303\n",
      "(Iteration 336 / 1960) loss: 1.430493\n",
      "(Iteration 341 / 1960) loss: 1.455875\n",
      "(Epoch 7 / 40) train acc: 0.491000; val_acc: 0.529000\n",
      "(Iteration 346 / 1960) loss: 1.442863\n",
      "(Iteration 351 / 1960) loss: 1.429819\n",
      "(Iteration 356 / 1960) loss: 1.417400\n",
      "(Iteration 361 / 1960) loss: 1.425329\n",
      "(Iteration 366 / 1960) loss: 1.407320\n",
      "(Iteration 371 / 1960) loss: 1.415856\n",
      "(Iteration 376 / 1960) loss: 1.425373\n",
      "(Iteration 381 / 1960) loss: 1.366635\n",
      "(Iteration 386 / 1960) loss: 1.392324\n",
      "(Iteration 391 / 1960) loss: 1.424482\n",
      "(Epoch 8 / 40) train acc: 0.527000; val_acc: 0.537000\n",
      "(Iteration 396 / 1960) loss: 1.370969\n",
      "(Iteration 401 / 1960) loss: 1.354648\n",
      "(Iteration 406 / 1960) loss: 1.345627\n",
      "(Iteration 411 / 1960) loss: 1.366428\n",
      "(Iteration 416 / 1960) loss: 1.372632\n",
      "(Iteration 421 / 1960) loss: 1.345228\n",
      "(Iteration 426 / 1960) loss: 1.313304\n",
      "(Iteration 431 / 1960) loss: 1.327331\n",
      "(Iteration 436 / 1960) loss: 1.346448\n",
      "(Iteration 441 / 1960) loss: 1.319310\n",
      "(Epoch 9 / 40) train acc: 0.564000; val_acc: 0.578000\n",
      "(Iteration 446 / 1960) loss: 1.304851\n",
      "(Iteration 451 / 1960) loss: 1.297481\n",
      "(Iteration 456 / 1960) loss: 1.348405\n",
      "(Iteration 461 / 1960) loss: 1.376651\n",
      "(Iteration 466 / 1960) loss: 1.356258\n",
      "(Iteration 471 / 1960) loss: 1.302468\n",
      "(Iteration 476 / 1960) loss: 1.294423\n",
      "(Iteration 481 / 1960) loss: 1.299364\n",
      "(Iteration 486 / 1960) loss: 1.242028\n",
      "(Epoch 10 / 40) train acc: 0.589000; val_acc: 0.575000\n",
      "(Iteration 491 / 1960) loss: 1.294944\n",
      "(Iteration 496 / 1960) loss: 1.326285\n",
      "(Iteration 501 / 1960) loss: 1.237771\n",
      "(Iteration 506 / 1960) loss: 1.336146\n",
      "(Iteration 511 / 1960) loss: 1.274801\n",
      "(Iteration 516 / 1960) loss: 1.236296\n",
      "(Iteration 521 / 1960) loss: 1.331520\n",
      "(Iteration 526 / 1960) loss: 1.254269\n",
      "(Iteration 531 / 1960) loss: 1.271799\n",
      "(Iteration 536 / 1960) loss: 1.187301\n",
      "(Epoch 11 / 40) train acc: 0.576000; val_acc: 0.578000\n",
      "(Iteration 541 / 1960) loss: 1.284829\n",
      "(Iteration 546 / 1960) loss: 1.238146\n",
      "(Iteration 551 / 1960) loss: 1.241494\n",
      "(Iteration 556 / 1960) loss: 1.314553\n",
      "(Iteration 561 / 1960) loss: 1.204684\n",
      "(Iteration 566 / 1960) loss: 1.200809\n",
      "(Iteration 571 / 1960) loss: 1.270200\n",
      "(Iteration 576 / 1960) loss: 1.281915\n",
      "(Iteration 581 / 1960) loss: 1.231230\n",
      "(Iteration 586 / 1960) loss: 1.200167\n",
      "(Epoch 12 / 40) train acc: 0.578000; val_acc: 0.597000\n",
      "(Iteration 591 / 1960) loss: 1.194429\n",
      "(Iteration 596 / 1960) loss: 1.231971\n",
      "(Iteration 601 / 1960) loss: 1.260844\n",
      "(Iteration 606 / 1960) loss: 1.224326\n",
      "(Iteration 611 / 1960) loss: 1.243357\n",
      "(Iteration 616 / 1960) loss: 1.210213\n",
      "(Iteration 621 / 1960) loss: 1.172104\n",
      "(Iteration 626 / 1960) loss: 1.202292\n",
      "(Iteration 631 / 1960) loss: 1.211297\n",
      "(Iteration 636 / 1960) loss: 1.162432\n",
      "(Epoch 13 / 40) train acc: 0.617000; val_acc: 0.594000\n",
      "(Iteration 641 / 1960) loss: 1.201963\n",
      "(Iteration 646 / 1960) loss: 1.193530\n",
      "(Iteration 651 / 1960) loss: 1.148800\n",
      "(Iteration 656 / 1960) loss: 1.196976\n",
      "(Iteration 661 / 1960) loss: 1.159652\n",
      "(Iteration 666 / 1960) loss: 1.214551\n",
      "(Iteration 671 / 1960) loss: 1.186920\n",
      "(Iteration 676 / 1960) loss: 1.175625\n",
      "(Iteration 681 / 1960) loss: 1.196951\n",
      "(Iteration 686 / 1960) loss: 1.125504\n",
      "(Epoch 14 / 40) train acc: 0.612000; val_acc: 0.615000\n",
      "(Iteration 691 / 1960) loss: 1.172341\n",
      "(Iteration 696 / 1960) loss: 1.170459\n",
      "(Iteration 701 / 1960) loss: 1.137880\n",
      "(Iteration 706 / 1960) loss: 1.156137\n",
      "(Iteration 711 / 1960) loss: 1.135652\n",
      "(Iteration 716 / 1960) loss: 1.118112\n",
      "(Iteration 721 / 1960) loss: 1.175387\n",
      "(Iteration 726 / 1960) loss: 1.128890\n",
      "(Iteration 731 / 1960) loss: 1.101070\n",
      "(Epoch 15 / 40) train acc: 0.607000; val_acc: 0.621000\n",
      "(Iteration 736 / 1960) loss: 1.133700\n",
      "(Iteration 741 / 1960) loss: 1.124108\n",
      "(Iteration 746 / 1960) loss: 1.180365\n",
      "(Iteration 751 / 1960) loss: 1.149096\n",
      "(Iteration 756 / 1960) loss: 1.094937\n",
      "(Iteration 761 / 1960) loss: 1.158405\n",
      "(Iteration 766 / 1960) loss: 1.168388\n",
      "(Iteration 771 / 1960) loss: 1.159896\n",
      "(Iteration 776 / 1960) loss: 1.177280\n",
      "(Iteration 781 / 1960) loss: 1.118343\n",
      "(Epoch 16 / 40) train acc: 0.626000; val_acc: 0.636000\n",
      "(Iteration 786 / 1960) loss: 1.116524\n",
      "(Iteration 791 / 1960) loss: 1.086738\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-682aca7688e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#                 verbose=True, print_every=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml2018/assignment2/cs231n/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;31m# Maybe print training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml2018/assignment2/cs231n/solver.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Compute loss and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml2018/assignment2/cs231n/classifiers/mycnn.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mdup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gamma2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"beta2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_bn_relu_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cacheconv2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mdup\u001b[0m   \u001b[0;34m=\u001b[0m  \u001b[0mmax_pool_backward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cachemaxPool1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mdup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gamma1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"beta1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_bn_relu_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cacheconv1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;31m#if para in grads:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml2018/assignment2/cs231n/layer_utils.py\u001b[0m in \u001b[0;36mconv_bn_relu_backward\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mdan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspatial_batchnorm_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_backward_fast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml2018/assignment2/cs231n/fast_layers.py\u001b[0m in \u001b[0;36mconv_backward_strides\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdout_reshaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mdx_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mdx_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol2im_6d_cython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdx_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# batch normalization and dropout useful. Store your best model in the         #\n",
    "# best_model variable.                                                         #\n",
    "################################################################################\n",
    "model = MyCNN([512, 256, 256, 128, 128,64], weight_scale=4e-3,reg = 0.0002)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                print_every = 20,\n",
    "                  num_epochs=40, batch_size=64,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 3e-4\n",
    "                  },\n",
    "                  verbose=True)\n",
    "# num_train = 1000\n",
    "# small_data = {\n",
    "#   'X_train': data['X_train'][:num_train],\n",
    "#   'y_train': data['y_train'][:num_train],\n",
    "#   'X_val': data['X_val'],\n",
    "#   'y_val': data['y_val'],\n",
    "# }\n",
    "\n",
    "\n",
    "# solver = Solver(model, small_data,\n",
    "#                 num_epochs=100, batch_size=50,\n",
    "#                 update_rule='adam',\n",
    "#                 optim_config={\n",
    "#                   'learning_rate': 1e-3,\n",
    "#                 },\n",
    "#                 verbose=True, print_every=1)\n",
    "\n",
    "solver.train()\n",
    "solver.train()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(solver.loss_history)\n",
    "plt.title(\"loss history\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(solver.train_acc_history,label = 'train')\n",
    "plt.plot(solver.val_acc_history,label = 'val')\n",
    "plt.title(\"accuracy history\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "best_model = model\n",
    "##############\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
