{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "y_test:  (1000,)\n",
      "y_train:  (49000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "y_val:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.mycnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 30600) loss: 2.416425\n",
      "(Epoch 0 / 40) train acc: 0.103000; val_acc: 0.102000\n",
      "(Iteration 21 / 30600) loss: 2.413968\n",
      "(Iteration 41 / 30600) loss: 2.411658\n",
      "(Iteration 61 / 30600) loss: 2.408543\n",
      "(Iteration 81 / 30600) loss: 2.406464\n",
      "(Iteration 101 / 30600) loss: 2.404398\n",
      "(Iteration 121 / 30600) loss: 2.403236\n",
      "(Iteration 141 / 30600) loss: 2.399745\n",
      "(Iteration 161 / 30600) loss: 2.397320\n",
      "(Iteration 181 / 30600) loss: 2.395126\n",
      "(Iteration 201 / 30600) loss: 2.392696\n",
      "(Iteration 221 / 30600) loss: 2.391134\n",
      "(Iteration 241 / 30600) loss: 2.389609\n",
      "(Iteration 261 / 30600) loss: 2.388006\n",
      "(Iteration 281 / 30600) loss: 2.386472\n",
      "(Iteration 301 / 30600) loss: 2.384561\n",
      "(Iteration 321 / 30600) loss: 2.383149\n",
      "(Iteration 341 / 30600) loss: 2.380930\n",
      "(Iteration 361 / 30600) loss: 2.379808\n",
      "(Iteration 381 / 30600) loss: 2.378152\n",
      "(Iteration 401 / 30600) loss: 2.377262\n",
      "(Iteration 421 / 30600) loss: 2.375372\n",
      "(Iteration 441 / 30600) loss: 2.373930\n",
      "(Iteration 461 / 30600) loss: 2.372253\n",
      "(Iteration 481 / 30600) loss: 2.370811\n",
      "(Iteration 501 / 30600) loss: 2.369471\n",
      "(Iteration 521 / 30600) loss: 2.368648\n",
      "(Iteration 541 / 30600) loss: 2.366989\n",
      "(Iteration 561 / 30600) loss: 2.366391\n",
      "(Iteration 581 / 30600) loss: 2.365615\n",
      "(Iteration 601 / 30600) loss: 2.363220\n",
      "(Iteration 621 / 30600) loss: 2.362500\n",
      "(Iteration 641 / 30600) loss: 2.361299\n",
      "(Iteration 661 / 30600) loss: 2.360939\n",
      "(Iteration 681 / 30600) loss: 2.359758\n",
      "(Iteration 701 / 30600) loss: 2.359328\n",
      "(Iteration 721 / 30600) loss: 2.357888\n",
      "(Iteration 741 / 30600) loss: 2.357729\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# batch normalization and dropout useful. Store your best model in the         #\n",
    "# best_model variable.                                                         #\n",
    "################################################################################\n",
    "model = MyCNN3([512, 256, 256, 128, 128,64], weight_scale=4e-3,reg = 0.0002)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                print_every = 20,\n",
    "                  num_epochs=40, batch_size=64,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 3e-4\n",
    "                  },\n",
    "                  verbose=True)\n",
    "# num_train = 1000\n",
    "# small_data = {\n",
    "#   'X_train': data['X_train'][:num_train],\n",
    "#   'y_train': data['y_train'][:num_train],\n",
    "#   'X_val': data['X_val'],\n",
    "#   'y_val': data['y_val'],\n",
    "# }\n",
    "\n",
    "\n",
    "# solver = Solver(model, small_data,\n",
    "#                 num_epochs=100, batch_size=50,\n",
    "#                 update_rule='adam',\n",
    "#                 optim_config={\n",
    "#                   'learning_rate': 1e-3,\n",
    "#                 },\n",
    "#                 verbose=True, print_every=1)\n",
    "\n",
    "solver.train()\n",
    "solver.train()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(solver.loss_history)\n",
    "plt.title(\"loss history\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(solver.train_acc_history,label = 'train')\n",
    "plt.plot(solver.val_acc_history,label = 'val')\n",
    "plt.title(\"accuracy history\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "best_model = model\n",
    "##############\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
