{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "X_val:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.mycnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 30600) loss: 2.402612\n",
      "(Epoch 0 / 40) train acc: 0.112000; val_acc: 0.098000\n",
      "(Iteration 21 / 30600) loss: 2.382062\n",
      "(Iteration 41 / 30600) loss: 2.328336\n",
      "(Iteration 61 / 30600) loss: 1.993578\n",
      "(Iteration 81 / 30600) loss: 1.911313\n",
      "(Iteration 101 / 30600) loss: 1.810124\n",
      "(Iteration 121 / 30600) loss: 1.582173\n",
      "(Iteration 141 / 30600) loss: 1.697092\n",
      "(Iteration 161 / 30600) loss: 1.622541\n",
      "(Iteration 181 / 30600) loss: 1.736443\n",
      "(Iteration 201 / 30600) loss: 1.532476\n",
      "(Iteration 221 / 30600) loss: 1.713531\n",
      "(Iteration 241 / 30600) loss: 1.585742\n",
      "(Iteration 261 / 30600) loss: 1.588845\n",
      "(Iteration 281 / 30600) loss: 1.392062\n",
      "(Iteration 301 / 30600) loss: 1.518150\n",
      "(Iteration 321 / 30600) loss: 1.686781\n",
      "(Iteration 341 / 30600) loss: 1.456709\n",
      "(Iteration 361 / 30600) loss: 1.377866\n",
      "(Iteration 381 / 30600) loss: 1.495240\n",
      "(Iteration 401 / 30600) loss: 1.239146\n",
      "(Iteration 421 / 30600) loss: 1.469460\n",
      "(Iteration 441 / 30600) loss: 1.459625\n",
      "(Iteration 461 / 30600) loss: 1.464185\n",
      "(Iteration 481 / 30600) loss: 1.340173\n",
      "(Iteration 501 / 30600) loss: 1.318160\n",
      "(Iteration 521 / 30600) loss: 1.314843\n",
      "(Iteration 541 / 30600) loss: 1.404122\n",
      "(Iteration 561 / 30600) loss: 1.309013\n",
      "(Iteration 581 / 30600) loss: 1.410458\n",
      "(Iteration 601 / 30600) loss: 1.370051\n",
      "(Iteration 621 / 30600) loss: 1.264033\n",
      "(Iteration 641 / 30600) loss: 1.415245\n",
      "(Iteration 661 / 30600) loss: 1.135557\n",
      "(Iteration 681 / 30600) loss: 1.108969\n",
      "(Iteration 701 / 30600) loss: 1.206049\n",
      "(Iteration 721 / 30600) loss: 0.977728\n",
      "(Iteration 741 / 30600) loss: 1.076456\n",
      "(Iteration 761 / 30600) loss: 1.071367\n",
      "(Epoch 1 / 40) train acc: 0.610000; val_acc: 0.618000\n",
      "(Iteration 781 / 30600) loss: 1.134308\n",
      "(Iteration 801 / 30600) loss: 1.374566\n",
      "(Iteration 821 / 30600) loss: 1.132811\n",
      "(Iteration 841 / 30600) loss: 1.199578\n",
      "(Iteration 861 / 30600) loss: 1.101832\n",
      "(Iteration 881 / 30600) loss: 1.075425\n",
      "(Iteration 901 / 30600) loss: 1.360839\n",
      "(Iteration 921 / 30600) loss: 1.139539\n",
      "(Iteration 941 / 30600) loss: 1.115342\n",
      "(Iteration 961 / 30600) loss: 1.221746\n",
      "(Iteration 981 / 30600) loss: 1.068423\n",
      "(Iteration 1001 / 30600) loss: 0.928262\n",
      "(Iteration 1021 / 30600) loss: 1.164963\n",
      "(Iteration 1041 / 30600) loss: 1.101696\n",
      "(Iteration 1061 / 30600) loss: 1.470483\n",
      "(Iteration 1081 / 30600) loss: 1.126740\n",
      "(Iteration 1101 / 30600) loss: 1.096486\n",
      "(Iteration 1121 / 30600) loss: 1.079272\n",
      "(Iteration 1141 / 30600) loss: 1.169109\n",
      "(Iteration 1161 / 30600) loss: 1.218047\n",
      "(Iteration 1181 / 30600) loss: 1.184362\n",
      "(Iteration 1201 / 30600) loss: 1.063724\n",
      "(Iteration 1221 / 30600) loss: 1.073024\n",
      "(Iteration 1241 / 30600) loss: 1.059243\n",
      "(Iteration 1261 / 30600) loss: 1.097246\n",
      "(Iteration 1281 / 30600) loss: 1.245499\n",
      "(Iteration 1301 / 30600) loss: 1.065785\n",
      "(Iteration 1321 / 30600) loss: 1.039752\n",
      "(Iteration 1341 / 30600) loss: 1.191494\n",
      "(Iteration 1361 / 30600) loss: 0.970532\n",
      "(Iteration 1381 / 30600) loss: 0.856439\n",
      "(Iteration 1401 / 30600) loss: 0.923150\n",
      "(Iteration 1421 / 30600) loss: 0.956474\n",
      "(Iteration 1441 / 30600) loss: 0.802984\n",
      "(Iteration 1461 / 30600) loss: 0.930371\n",
      "(Iteration 1481 / 30600) loss: 1.255165\n",
      "(Iteration 1501 / 30600) loss: 1.089176\n",
      "(Iteration 1521 / 30600) loss: 0.860537\n",
      "(Epoch 2 / 40) train acc: 0.665000; val_acc: 0.659000\n",
      "(Iteration 1541 / 30600) loss: 1.092293\n",
      "(Iteration 1561 / 30600) loss: 0.885829\n",
      "(Iteration 1581 / 30600) loss: 1.018100\n",
      "(Iteration 1601 / 30600) loss: 0.965998\n",
      "(Iteration 1621 / 30600) loss: 1.187508\n",
      "(Iteration 1641 / 30600) loss: 1.027539\n",
      "(Iteration 1661 / 30600) loss: 0.914995\n",
      "(Iteration 1681 / 30600) loss: 0.970157\n",
      "(Iteration 1701 / 30600) loss: 1.048472\n",
      "(Iteration 1721 / 30600) loss: 0.892565\n",
      "(Iteration 1741 / 30600) loss: 1.099373\n",
      "(Iteration 1761 / 30600) loss: 0.931776\n",
      "(Iteration 1781 / 30600) loss: 0.885112\n",
      "(Iteration 1801 / 30600) loss: 1.033208\n",
      "(Iteration 1821 / 30600) loss: 1.058454\n",
      "(Iteration 1841 / 30600) loss: 0.928733\n",
      "(Iteration 1861 / 30600) loss: 0.838963\n",
      "(Iteration 1881 / 30600) loss: 0.946962\n",
      "(Iteration 1901 / 30600) loss: 0.895730\n",
      "(Iteration 1921 / 30600) loss: 0.961690\n",
      "(Iteration 1941 / 30600) loss: 0.807339\n",
      "(Iteration 1961 / 30600) loss: 0.838627\n",
      "(Iteration 1981 / 30600) loss: 0.802090\n",
      "(Iteration 2001 / 30600) loss: 1.031815\n",
      "(Iteration 2021 / 30600) loss: 0.776084\n",
      "(Iteration 2041 / 30600) loss: 0.709947\n",
      "(Iteration 2061 / 30600) loss: 0.897100\n",
      "(Iteration 2081 / 30600) loss: 1.010087\n",
      "(Iteration 2101 / 30600) loss: 0.939366\n",
      "(Iteration 2121 / 30600) loss: 1.044750\n",
      "(Iteration 2141 / 30600) loss: 1.053318\n",
      "(Iteration 2161 / 30600) loss: 0.880638\n",
      "(Iteration 2181 / 30600) loss: 0.900277\n",
      "(Iteration 2201 / 30600) loss: 0.907249\n",
      "(Iteration 2221 / 30600) loss: 0.737636\n",
      "(Iteration 2241 / 30600) loss: 1.123291\n",
      "(Iteration 2261 / 30600) loss: 0.606419\n",
      "(Iteration 2281 / 30600) loss: 1.171363\n",
      "(Epoch 3 / 40) train acc: 0.712000; val_acc: 0.686000\n",
      "(Iteration 2301 / 30600) loss: 1.052040\n",
      "(Iteration 2321 / 30600) loss: 1.059013\n",
      "(Iteration 2341 / 30600) loss: 1.177937\n",
      "(Iteration 2361 / 30600) loss: 0.863621\n",
      "(Iteration 2381 / 30600) loss: 0.899466\n",
      "(Iteration 2401 / 30600) loss: 1.021258\n",
      "(Iteration 2421 / 30600) loss: 1.086414\n",
      "(Iteration 2441 / 30600) loss: 0.783401\n",
      "(Iteration 2461 / 30600) loss: 1.070861\n",
      "(Iteration 2481 / 30600) loss: 0.978827\n",
      "(Iteration 2501 / 30600) loss: 0.771368\n",
      "(Iteration 2521 / 30600) loss: 1.030840\n",
      "(Iteration 2541 / 30600) loss: 0.877648\n",
      "(Iteration 2561 / 30600) loss: 0.931724\n",
      "(Iteration 2581 / 30600) loss: 1.023433\n",
      "(Iteration 2601 / 30600) loss: 0.853192\n",
      "(Iteration 2621 / 30600) loss: 0.868528\n",
      "(Iteration 2641 / 30600) loss: 0.743572\n",
      "(Iteration 2661 / 30600) loss: 0.831695\n",
      "(Iteration 2681 / 30600) loss: 0.886602\n",
      "(Iteration 2701 / 30600) loss: 0.622718\n",
      "(Iteration 2721 / 30600) loss: 0.915432\n",
      "(Iteration 2741 / 30600) loss: 0.880800\n",
      "(Iteration 2761 / 30600) loss: 0.770020\n",
      "(Iteration 2781 / 30600) loss: 0.780166\n",
      "(Iteration 2801 / 30600) loss: 0.758056\n",
      "(Iteration 2821 / 30600) loss: 0.777155\n",
      "(Iteration 2841 / 30600) loss: 0.924376\n",
      "(Iteration 2861 / 30600) loss: 0.698338\n",
      "(Iteration 2881 / 30600) loss: 0.771339\n",
      "(Iteration 2901 / 30600) loss: 0.856100\n",
      "(Iteration 2921 / 30600) loss: 0.774528\n",
      "(Iteration 2941 / 30600) loss: 0.923760\n",
      "(Iteration 2961 / 30600) loss: 0.713641\n",
      "(Iteration 2981 / 30600) loss: 0.863139\n",
      "(Iteration 3001 / 30600) loss: 0.888864\n",
      "(Iteration 3021 / 30600) loss: 0.932758\n",
      "(Iteration 3041 / 30600) loss: 0.776798\n",
      "(Epoch 4 / 40) train acc: 0.749000; val_acc: 0.691000\n",
      "(Iteration 3061 / 30600) loss: 1.124573\n",
      "(Iteration 3081 / 30600) loss: 0.690568\n",
      "(Iteration 3101 / 30600) loss: 0.942517\n",
      "(Iteration 3121 / 30600) loss: 0.833788\n",
      "(Iteration 3141 / 30600) loss: 0.717834\n",
      "(Iteration 3161 / 30600) loss: 0.604988\n",
      "(Iteration 3181 / 30600) loss: 0.891139\n",
      "(Iteration 3201 / 30600) loss: 0.820500\n",
      "(Iteration 3221 / 30600) loss: 0.782410\n",
      "(Iteration 3241 / 30600) loss: 0.923333\n",
      "(Iteration 3261 / 30600) loss: 0.782841\n",
      "(Iteration 3281 / 30600) loss: 1.026657\n",
      "(Iteration 3301 / 30600) loss: 0.716024\n",
      "(Iteration 3321 / 30600) loss: 0.786428\n",
      "(Iteration 3341 / 30600) loss: 0.706575\n",
      "(Iteration 3361 / 30600) loss: 0.670697\n",
      "(Iteration 3381 / 30600) loss: 0.893082\n",
      "(Iteration 3401 / 30600) loss: 0.833271\n",
      "(Iteration 3421 / 30600) loss: 0.718728\n",
      "(Iteration 3441 / 30600) loss: 0.941595\n",
      "(Iteration 3461 / 30600) loss: 0.719387\n",
      "(Iteration 3481 / 30600) loss: 0.879002\n",
      "(Iteration 3501 / 30600) loss: 0.639426\n",
      "(Iteration 3521 / 30600) loss: 0.754358\n",
      "(Iteration 3541 / 30600) loss: 0.899183\n",
      "(Iteration 3561 / 30600) loss: 0.861663\n",
      "(Iteration 3581 / 30600) loss: 0.522820\n",
      "(Iteration 3601 / 30600) loss: 0.769724\n",
      "(Iteration 3621 / 30600) loss: 0.779762\n",
      "(Iteration 3641 / 30600) loss: 0.759628\n",
      "(Iteration 3661 / 30600) loss: 0.627651\n",
      "(Iteration 3681 / 30600) loss: 0.869660\n",
      "(Iteration 3701 / 30600) loss: 0.701105\n",
      "(Iteration 3721 / 30600) loss: 0.870385\n",
      "(Iteration 3741 / 30600) loss: 0.791057\n",
      "(Iteration 3761 / 30600) loss: 0.717130\n",
      "(Iteration 3781 / 30600) loss: 0.631868\n",
      "(Iteration 3801 / 30600) loss: 0.898753\n",
      "(Iteration 3821 / 30600) loss: 0.708993\n",
      "(Epoch 5 / 40) train acc: 0.779000; val_acc: 0.728000\n",
      "(Iteration 3841 / 30600) loss: 0.849318\n",
      "(Iteration 3861 / 30600) loss: 0.952536\n",
      "(Iteration 3881 / 30600) loss: 0.718333\n",
      "(Iteration 3901 / 30600) loss: 0.586353\n",
      "(Iteration 3921 / 30600) loss: 0.729490\n",
      "(Iteration 3941 / 30600) loss: 0.703170\n",
      "(Iteration 3961 / 30600) loss: 0.801392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 3981 / 30600) loss: 0.648977\n",
      "(Iteration 4001 / 30600) loss: 0.585824\n",
      "(Iteration 4021 / 30600) loss: 0.546051\n",
      "(Iteration 4041 / 30600) loss: 0.581331\n",
      "(Iteration 4061 / 30600) loss: 0.863129\n",
      "(Iteration 4081 / 30600) loss: 0.871603\n",
      "(Iteration 4101 / 30600) loss: 0.746855\n",
      "(Iteration 4121 / 30600) loss: 0.629734\n",
      "(Iteration 4141 / 30600) loss: 0.805273\n",
      "(Iteration 4161 / 30600) loss: 0.700004\n",
      "(Iteration 4181 / 30600) loss: 0.654839\n",
      "(Iteration 4201 / 30600) loss: 0.618177\n",
      "(Iteration 4221 / 30600) loss: 0.867979\n",
      "(Iteration 4241 / 30600) loss: 0.606136\n",
      "(Iteration 4261 / 30600) loss: 0.748905\n",
      "(Iteration 4281 / 30600) loss: 0.916423\n",
      "(Iteration 4301 / 30600) loss: 0.772486\n",
      "(Iteration 4321 / 30600) loss: 0.796798\n",
      "(Iteration 4341 / 30600) loss: 0.622237\n",
      "(Iteration 4361 / 30600) loss: 0.776661\n",
      "(Iteration 4381 / 30600) loss: 0.627711\n",
      "(Iteration 4401 / 30600) loss: 0.560968\n",
      "(Iteration 4421 / 30600) loss: 0.456217\n",
      "(Iteration 4441 / 30600) loss: 0.869132\n",
      "(Iteration 4461 / 30600) loss: 0.678871\n",
      "(Iteration 4481 / 30600) loss: 0.669498\n",
      "(Iteration 4501 / 30600) loss: 0.644062\n",
      "(Iteration 4521 / 30600) loss: 0.834998\n",
      "(Iteration 4541 / 30600) loss: 0.587656\n",
      "(Iteration 4561 / 30600) loss: 0.531611\n",
      "(Iteration 4581 / 30600) loss: 0.731435\n",
      "(Epoch 6 / 40) train acc: 0.830000; val_acc: 0.740000\n",
      "(Iteration 4601 / 30600) loss: 0.577815\n",
      "(Iteration 4621 / 30600) loss: 0.569127\n",
      "(Iteration 4641 / 30600) loss: 0.697462\n",
      "(Iteration 4661 / 30600) loss: 0.664251\n",
      "(Iteration 4681 / 30600) loss: 0.637773\n",
      "(Iteration 4701 / 30600) loss: 0.575240\n",
      "(Iteration 4721 / 30600) loss: 0.685798\n",
      "(Iteration 4741 / 30600) loss: 0.822304\n",
      "(Iteration 4761 / 30600) loss: 0.703917\n",
      "(Iteration 4781 / 30600) loss: 0.709053\n",
      "(Iteration 4801 / 30600) loss: 0.699624\n",
      "(Iteration 4821 / 30600) loss: 0.762118\n",
      "(Iteration 4841 / 30600) loss: 0.611070\n",
      "(Iteration 4861 / 30600) loss: 0.751882\n",
      "(Iteration 4881 / 30600) loss: 0.628390\n",
      "(Iteration 4901 / 30600) loss: 0.734926\n",
      "(Iteration 4921 / 30600) loss: 0.582220\n",
      "(Iteration 4941 / 30600) loss: 0.832150\n",
      "(Iteration 4961 / 30600) loss: 0.587673\n",
      "(Iteration 4981 / 30600) loss: 0.689038\n",
      "(Iteration 5001 / 30600) loss: 0.765260\n",
      "(Iteration 5021 / 30600) loss: 0.780791\n",
      "(Iteration 5041 / 30600) loss: 0.543507\n",
      "(Iteration 5061 / 30600) loss: 0.628784\n",
      "(Iteration 5081 / 30600) loss: 0.745867\n",
      "(Iteration 5101 / 30600) loss: 0.546174\n",
      "(Iteration 5121 / 30600) loss: 0.814060\n",
      "(Iteration 5141 / 30600) loss: 0.687567\n",
      "(Iteration 5161 / 30600) loss: 0.772309\n",
      "(Iteration 5181 / 30600) loss: 0.788095\n",
      "(Iteration 5201 / 30600) loss: 0.780442\n",
      "(Iteration 5221 / 30600) loss: 0.654795\n",
      "(Iteration 5241 / 30600) loss: 0.560205\n",
      "(Iteration 5261 / 30600) loss: 0.607254\n",
      "(Iteration 5281 / 30600) loss: 0.685789\n",
      "(Iteration 5301 / 30600) loss: 0.560297\n",
      "(Iteration 5321 / 30600) loss: 0.585046\n",
      "(Iteration 5341 / 30600) loss: 0.639484\n",
      "(Epoch 7 / 40) train acc: 0.826000; val_acc: 0.741000\n",
      "(Iteration 5361 / 30600) loss: 0.766623\n",
      "(Iteration 5381 / 30600) loss: 0.634882\n",
      "(Iteration 5401 / 30600) loss: 0.625054\n",
      "(Iteration 5421 / 30600) loss: 0.764880\n",
      "(Iteration 5441 / 30600) loss: 0.813214\n",
      "(Iteration 5461 / 30600) loss: 0.520784\n",
      "(Iteration 5481 / 30600) loss: 0.773627\n",
      "(Iteration 5501 / 30600) loss: 0.631974\n",
      "(Iteration 5521 / 30600) loss: 0.701851\n",
      "(Iteration 5541 / 30600) loss: 0.550306\n",
      "(Iteration 5561 / 30600) loss: 0.855219\n",
      "(Iteration 5581 / 30600) loss: 0.466126\n",
      "(Iteration 5601 / 30600) loss: 0.525378\n",
      "(Iteration 5621 / 30600) loss: 0.688556\n",
      "(Iteration 5641 / 30600) loss: 0.448326\n",
      "(Iteration 5661 / 30600) loss: 0.593874\n",
      "(Iteration 5681 / 30600) loss: 0.535603\n",
      "(Iteration 5701 / 30600) loss: 0.499684\n",
      "(Iteration 5721 / 30600) loss: 0.619188\n",
      "(Iteration 5741 / 30600) loss: 0.624499\n",
      "(Iteration 5761 / 30600) loss: 0.402112\n",
      "(Iteration 5781 / 30600) loss: 0.792894\n",
      "(Iteration 5801 / 30600) loss: 0.593536\n",
      "(Iteration 5821 / 30600) loss: 0.487304\n",
      "(Iteration 5841 / 30600) loss: 0.801554\n",
      "(Iteration 5861 / 30600) loss: 0.551272\n",
      "(Iteration 5881 / 30600) loss: 0.508321\n",
      "(Iteration 5901 / 30600) loss: 0.603453\n",
      "(Iteration 5921 / 30600) loss: 0.506938\n",
      "(Iteration 5941 / 30600) loss: 0.516895\n",
      "(Iteration 5961 / 30600) loss: 0.435991\n",
      "(Iteration 5981 / 30600) loss: 0.540346\n",
      "(Iteration 6001 / 30600) loss: 0.765890\n",
      "(Iteration 6021 / 30600) loss: 0.582522\n",
      "(Iteration 6041 / 30600) loss: 0.812038\n",
      "(Iteration 6061 / 30600) loss: 0.597271\n",
      "(Iteration 6081 / 30600) loss: 0.796526\n",
      "(Iteration 6101 / 30600) loss: 0.576024\n",
      "(Epoch 8 / 40) train acc: 0.853000; val_acc: 0.741000\n",
      "(Iteration 6121 / 30600) loss: 0.620045\n",
      "(Iteration 6141 / 30600) loss: 0.749898\n",
      "(Iteration 6161 / 30600) loss: 0.584005\n",
      "(Iteration 6181 / 30600) loss: 0.601286\n",
      "(Iteration 6201 / 30600) loss: 0.541022\n",
      "(Iteration 6221 / 30600) loss: 0.634186\n",
      "(Iteration 6241 / 30600) loss: 0.480870\n",
      "(Iteration 6261 / 30600) loss: 0.406948\n",
      "(Iteration 6281 / 30600) loss: 0.672651\n",
      "(Iteration 6301 / 30600) loss: 0.600985\n",
      "(Iteration 6321 / 30600) loss: 0.723169\n",
      "(Iteration 6341 / 30600) loss: 0.442258\n",
      "(Iteration 6361 / 30600) loss: 0.637191\n",
      "(Iteration 6381 / 30600) loss: 0.600588\n",
      "(Iteration 6401 / 30600) loss: 0.539322\n",
      "(Iteration 6421 / 30600) loss: 0.753529\n",
      "(Iteration 6441 / 30600) loss: 0.476765\n",
      "(Iteration 6461 / 30600) loss: 0.740747\n",
      "(Iteration 6481 / 30600) loss: 0.626880\n",
      "(Iteration 6501 / 30600) loss: 0.671460\n",
      "(Iteration 6521 / 30600) loss: 0.533634\n",
      "(Iteration 6541 / 30600) loss: 0.459885\n",
      "(Iteration 6561 / 30600) loss: 0.566480\n",
      "(Iteration 6581 / 30600) loss: 0.429654\n",
      "(Iteration 6601 / 30600) loss: 0.531346\n",
      "(Iteration 6621 / 30600) loss: 0.618375\n",
      "(Iteration 6641 / 30600) loss: 0.645291\n",
      "(Iteration 6661 / 30600) loss: 0.606854\n",
      "(Iteration 6681 / 30600) loss: 0.549060\n",
      "(Iteration 6701 / 30600) loss: 0.595342\n",
      "(Iteration 6721 / 30600) loss: 0.587490\n",
      "(Iteration 6741 / 30600) loss: 0.605444\n",
      "(Iteration 6761 / 30600) loss: 0.445444\n",
      "(Iteration 6781 / 30600) loss: 0.490719\n",
      "(Iteration 6801 / 30600) loss: 0.556917\n",
      "(Iteration 6821 / 30600) loss: 0.677901\n",
      "(Iteration 6841 / 30600) loss: 0.465783\n",
      "(Iteration 6861 / 30600) loss: 0.641162\n",
      "(Iteration 6881 / 30600) loss: 0.533849\n",
      "(Epoch 9 / 40) train acc: 0.836000; val_acc: 0.726000\n",
      "(Iteration 6901 / 30600) loss: 0.566693\n",
      "(Iteration 6921 / 30600) loss: 0.632158\n",
      "(Iteration 6941 / 30600) loss: 0.506333\n",
      "(Iteration 6961 / 30600) loss: 0.449133\n",
      "(Iteration 6981 / 30600) loss: 0.519581\n",
      "(Iteration 7001 / 30600) loss: 0.399081\n",
      "(Iteration 7021 / 30600) loss: 0.608468\n",
      "(Iteration 7041 / 30600) loss: 0.651753\n",
      "(Iteration 7061 / 30600) loss: 0.624408\n",
      "(Iteration 7081 / 30600) loss: 0.582242\n",
      "(Iteration 7101 / 30600) loss: 0.660540\n",
      "(Iteration 7121 / 30600) loss: 0.460459\n",
      "(Iteration 7141 / 30600) loss: 0.625205\n",
      "(Iteration 7161 / 30600) loss: 0.494340\n",
      "(Iteration 7181 / 30600) loss: 0.432963\n",
      "(Iteration 7201 / 30600) loss: 0.496629\n",
      "(Iteration 7221 / 30600) loss: 0.458264\n",
      "(Iteration 7241 / 30600) loss: 0.529804\n",
      "(Iteration 7261 / 30600) loss: 0.446761\n",
      "(Iteration 7281 / 30600) loss: 0.516308\n",
      "(Iteration 7301 / 30600) loss: 0.557950\n",
      "(Iteration 7321 / 30600) loss: 0.439482\n",
      "(Iteration 7341 / 30600) loss: 0.552795\n",
      "(Iteration 7361 / 30600) loss: 0.458961\n",
      "(Iteration 7381 / 30600) loss: 0.480241\n",
      "(Iteration 7401 / 30600) loss: 0.546678\n",
      "(Iteration 7421 / 30600) loss: 0.535081\n",
      "(Iteration 7441 / 30600) loss: 0.430085\n",
      "(Iteration 7461 / 30600) loss: 0.609476\n",
      "(Iteration 7481 / 30600) loss: 0.473839\n",
      "(Iteration 7501 / 30600) loss: 0.630819\n",
      "(Iteration 7521 / 30600) loss: 0.496879\n",
      "(Iteration 7541 / 30600) loss: 0.498770\n",
      "(Iteration 7561 / 30600) loss: 0.474360\n",
      "(Iteration 7581 / 30600) loss: 0.404850\n",
      "(Iteration 7601 / 30600) loss: 0.698706\n",
      "(Iteration 7621 / 30600) loss: 0.492414\n",
      "(Iteration 7641 / 30600) loss: 0.569695\n",
      "(Epoch 10 / 40) train acc: 0.867000; val_acc: 0.746000\n",
      "(Iteration 7661 / 30600) loss: 0.572876\n",
      "(Iteration 7681 / 30600) loss: 0.680802\n",
      "(Iteration 7701 / 30600) loss: 0.465728\n",
      "(Iteration 7721 / 30600) loss: 0.502455\n",
      "(Iteration 7741 / 30600) loss: 0.479663\n",
      "(Iteration 7761 / 30600) loss: 0.715886\n",
      "(Iteration 7781 / 30600) loss: 0.591587\n",
      "(Iteration 7801 / 30600) loss: 0.515988\n",
      "(Iteration 7821 / 30600) loss: 0.527727\n",
      "(Iteration 7841 / 30600) loss: 0.458255\n",
      "(Iteration 7861 / 30600) loss: 0.541877\n",
      "(Iteration 7881 / 30600) loss: 0.480881\n",
      "(Iteration 7901 / 30600) loss: 0.584262\n",
      "(Iteration 7921 / 30600) loss: 0.508928\n",
      "(Iteration 7941 / 30600) loss: 0.631664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 7961 / 30600) loss: 0.608132\n",
      "(Iteration 7981 / 30600) loss: 0.632918\n",
      "(Iteration 8001 / 30600) loss: 0.583994\n",
      "(Iteration 8021 / 30600) loss: 0.428398\n",
      "(Iteration 8041 / 30600) loss: 0.588060\n",
      "(Iteration 8061 / 30600) loss: 0.376815\n",
      "(Iteration 8081 / 30600) loss: 0.423998\n",
      "(Iteration 8101 / 30600) loss: 0.420161\n",
      "(Iteration 8121 / 30600) loss: 0.528944\n",
      "(Iteration 8141 / 30600) loss: 0.426407\n",
      "(Iteration 8161 / 30600) loss: 0.589769\n",
      "(Iteration 8181 / 30600) loss: 0.467587\n",
      "(Iteration 8201 / 30600) loss: 0.567983\n",
      "(Iteration 8221 / 30600) loss: 0.538151\n",
      "(Iteration 8241 / 30600) loss: 0.605595\n",
      "(Iteration 8261 / 30600) loss: 0.679761\n",
      "(Iteration 8281 / 30600) loss: 0.390970\n",
      "(Iteration 8301 / 30600) loss: 0.480005\n",
      "(Iteration 8321 / 30600) loss: 0.460786\n",
      "(Iteration 8341 / 30600) loss: 0.472292\n",
      "(Iteration 8361 / 30600) loss: 0.513806\n",
      "(Iteration 8381 / 30600) loss: 0.539056\n",
      "(Iteration 8401 / 30600) loss: 0.440135\n",
      "(Epoch 11 / 40) train acc: 0.889000; val_acc: 0.754000\n",
      "(Iteration 8421 / 30600) loss: 0.407926\n",
      "(Iteration 8441 / 30600) loss: 0.561283\n",
      "(Iteration 8461 / 30600) loss: 0.498749\n",
      "(Iteration 8481 / 30600) loss: 0.635418\n",
      "(Iteration 8501 / 30600) loss: 0.502168\n",
      "(Iteration 8521 / 30600) loss: 0.575756\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# batch normalization and dropout useful. Store your best model in the         #\n",
    "# best_model variable.                                                         #\n",
    "################################################################################\n",
    "model = MyCNN([512, 256, 256, 128, 128,64], weight_scale=4e-3,reg = 0.0002)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                print_every = 20,\n",
    "                  num_epochs=40, batch_size=64,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 3e-4\n",
    "                  },\n",
    "                  verbose=True)\n",
    "# num_train = 1000\n",
    "# small_data = {\n",
    "#   'X_train': data['X_train'][:num_train],\n",
    "#   'y_train': data['y_train'][:num_train],\n",
    "#   'X_val': data['X_val'],\n",
    "#   'y_val': data['y_val'],\n",
    "# }\n",
    "\n",
    "\n",
    "# solver = Solver(model, small_data,\n",
    "#                 num_epochs=100, batch_size=50,\n",
    "#                 update_rule='adam',\n",
    "#                 optim_config={\n",
    "#                   'learning_rate': 1e-3,\n",
    "#                 },\n",
    "#                 verbose=True, print_every=1)\n",
    "\n",
    "solver.train()\n",
    "solver.train()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(solver.loss_history)\n",
    "plt.title(\"loss history\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(solver.train_acc_history,label = 'train')\n",
    "plt.plot(solver.val_acc_history,label = 'val')\n",
    "plt.title(\"accuracy history\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "best_model = model\n",
    "##############\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
