{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_val:  (1000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.mycnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "  print('%s: ' % k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 30600) loss: 2.416160\n",
      "(Epoch 0 / 40) train acc: 0.083000; val_acc: 0.113000\n",
      "(Iteration 21 / 30600) loss: 2.412210\n",
      "(Iteration 41 / 30600) loss: 2.409397\n",
      "(Iteration 61 / 30600) loss: 2.402432\n",
      "(Iteration 81 / 30600) loss: 2.336681\n",
      "(Iteration 101 / 30600) loss: 2.427616\n",
      "(Iteration 121 / 30600) loss: 2.175074\n",
      "(Iteration 141 / 30600) loss: 2.187037\n",
      "(Iteration 161 / 30600) loss: 2.000787\n",
      "(Iteration 181 / 30600) loss: 2.012163\n",
      "(Iteration 201 / 30600) loss: 1.921275\n",
      "(Iteration 221 / 30600) loss: 1.814169\n",
      "(Iteration 241 / 30600) loss: 1.858515\n",
      "(Iteration 261 / 30600) loss: 1.783986\n",
      "(Iteration 281 / 30600) loss: 1.640473\n",
      "(Iteration 301 / 30600) loss: 1.926713\n",
      "(Iteration 321 / 30600) loss: 1.799090\n",
      "(Iteration 341 / 30600) loss: 1.812479\n",
      "(Iteration 361 / 30600) loss: 1.665582\n",
      "(Iteration 381 / 30600) loss: 1.772868\n",
      "(Iteration 401 / 30600) loss: 1.644560\n",
      "(Iteration 421 / 30600) loss: 1.484296\n",
      "(Iteration 441 / 30600) loss: 1.530131\n",
      "(Iteration 461 / 30600) loss: 1.416693\n",
      "(Iteration 481 / 30600) loss: 1.509490\n",
      "(Iteration 501 / 30600) loss: 1.650640\n",
      "(Iteration 521 / 30600) loss: 1.571623\n",
      "(Iteration 541 / 30600) loss: 1.596842\n",
      "(Iteration 561 / 30600) loss: 1.447508\n",
      "(Iteration 581 / 30600) loss: 1.474633\n",
      "(Iteration 601 / 30600) loss: 1.359682\n",
      "(Iteration 621 / 30600) loss: 1.253321\n",
      "(Iteration 641 / 30600) loss: 1.267782\n",
      "(Iteration 661 / 30600) loss: 1.340529\n",
      "(Iteration 681 / 30600) loss: 1.529476\n",
      "(Iteration 701 / 30600) loss: 1.564135\n",
      "(Iteration 721 / 30600) loss: 1.654199\n",
      "(Iteration 741 / 30600) loss: 1.282863\n",
      "(Iteration 761 / 30600) loss: 1.411537\n",
      "(Epoch 1 / 40) train acc: 0.554000; val_acc: 0.535000\n",
      "(Iteration 781 / 30600) loss: 1.464272\n",
      "(Iteration 801 / 30600) loss: 1.535833\n",
      "(Iteration 821 / 30600) loss: 1.434774\n",
      "(Iteration 841 / 30600) loss: 1.427991\n",
      "(Iteration 861 / 30600) loss: 1.600740\n",
      "(Iteration 881 / 30600) loss: 1.109677\n",
      "(Iteration 901 / 30600) loss: 1.225208\n",
      "(Iteration 921 / 30600) loss: 1.217077\n",
      "(Iteration 941 / 30600) loss: 1.201648\n",
      "(Iteration 961 / 30600) loss: 1.467727\n",
      "(Iteration 981 / 30600) loss: 1.355696\n",
      "(Iteration 1001 / 30600) loss: 1.095195\n",
      "(Iteration 1021 / 30600) loss: 1.360894\n",
      "(Iteration 1041 / 30600) loss: 1.585591\n",
      "(Iteration 1061 / 30600) loss: 1.471969\n",
      "(Iteration 1081 / 30600) loss: 1.284947\n",
      "(Iteration 1101 / 30600) loss: 1.092462\n",
      "(Iteration 1121 / 30600) loss: 1.199071\n",
      "(Iteration 1141 / 30600) loss: 1.284055\n",
      "(Iteration 1161 / 30600) loss: 1.076190\n",
      "(Iteration 1181 / 30600) loss: 1.288604\n",
      "(Iteration 1201 / 30600) loss: 1.285289\n",
      "(Iteration 1221 / 30600) loss: 1.143558\n",
      "(Iteration 1241 / 30600) loss: 1.268931\n",
      "(Iteration 1261 / 30600) loss: 1.145517\n",
      "(Iteration 1281 / 30600) loss: 1.345059\n",
      "(Iteration 1301 / 30600) loss: 1.122685\n",
      "(Iteration 1321 / 30600) loss: 1.140545\n",
      "(Iteration 1341 / 30600) loss: 1.398511\n",
      "(Iteration 1361 / 30600) loss: 1.144950\n",
      "(Iteration 1381 / 30600) loss: 1.055280\n",
      "(Iteration 1401 / 30600) loss: 1.326456\n",
      "(Iteration 1421 / 30600) loss: 1.000685\n",
      "(Iteration 1441 / 30600) loss: 1.091658\n",
      "(Iteration 1461 / 30600) loss: 1.092007\n",
      "(Iteration 1481 / 30600) loss: 1.185238\n",
      "(Iteration 1501 / 30600) loss: 1.020591\n",
      "(Iteration 1521 / 30600) loss: 1.573202\n",
      "(Epoch 2 / 40) train acc: 0.596000; val_acc: 0.591000\n",
      "(Iteration 1541 / 30600) loss: 1.384047\n",
      "(Iteration 1561 / 30600) loss: 1.053503\n",
      "(Iteration 1581 / 30600) loss: 1.264640\n",
      "(Iteration 1601 / 30600) loss: 0.966858\n",
      "(Iteration 1621 / 30600) loss: 1.278514\n",
      "(Iteration 1641 / 30600) loss: 1.065157\n",
      "(Iteration 1661 / 30600) loss: 1.358843\n",
      "(Iteration 1681 / 30600) loss: 0.905275\n",
      "(Iteration 1701 / 30600) loss: 1.287266\n",
      "(Iteration 1721 / 30600) loss: 1.132261\n",
      "(Iteration 1741 / 30600) loss: 1.088180\n",
      "(Iteration 1761 / 30600) loss: 1.080817\n",
      "(Iteration 1781 / 30600) loss: 1.158317\n",
      "(Iteration 1801 / 30600) loss: 0.954738\n",
      "(Iteration 1821 / 30600) loss: 1.098392\n",
      "(Iteration 1841 / 30600) loss: 1.165484\n",
      "(Iteration 1861 / 30600) loss: 1.011037\n",
      "(Iteration 1881 / 30600) loss: 1.082865\n",
      "(Iteration 1901 / 30600) loss: 1.291986\n",
      "(Iteration 1921 / 30600) loss: 1.167831\n",
      "(Iteration 1941 / 30600) loss: 1.112794\n",
      "(Iteration 1961 / 30600) loss: 0.929807\n",
      "(Iteration 1981 / 30600) loss: 1.155460\n",
      "(Iteration 2001 / 30600) loss: 1.214325\n",
      "(Iteration 2021 / 30600) loss: 1.090370\n",
      "(Iteration 2041 / 30600) loss: 0.995256\n",
      "(Iteration 2061 / 30600) loss: 1.115259\n",
      "(Iteration 2081 / 30600) loss: 1.304049\n",
      "(Iteration 2101 / 30600) loss: 1.057759\n",
      "(Iteration 2121 / 30600) loss: 1.025115\n",
      "(Iteration 2141 / 30600) loss: 1.009855\n",
      "(Iteration 2161 / 30600) loss: 0.956779\n",
      "(Iteration 2181 / 30600) loss: 1.165527\n",
      "(Iteration 2201 / 30600) loss: 1.052947\n",
      "(Iteration 2221 / 30600) loss: 0.937816\n",
      "(Iteration 2241 / 30600) loss: 1.052028\n",
      "(Iteration 2261 / 30600) loss: 0.832628\n",
      "(Iteration 2281 / 30600) loss: 1.115685\n",
      "(Epoch 3 / 40) train acc: 0.632000; val_acc: 0.632000\n",
      "(Iteration 2301 / 30600) loss: 1.080587\n",
      "(Iteration 2321 / 30600) loss: 1.166262\n",
      "(Iteration 2341 / 30600) loss: 1.155972\n",
      "(Iteration 2361 / 30600) loss: 0.979675\n",
      "(Iteration 2381 / 30600) loss: 1.021246\n",
      "(Iteration 2401 / 30600) loss: 1.104080\n",
      "(Iteration 2421 / 30600) loss: 1.314919\n",
      "(Iteration 2441 / 30600) loss: 1.204942\n",
      "(Iteration 2461 / 30600) loss: 1.002636\n",
      "(Iteration 2481 / 30600) loss: 1.024699\n",
      "(Iteration 2501 / 30600) loss: 1.116460\n",
      "(Iteration 2521 / 30600) loss: 1.161093\n",
      "(Iteration 2541 / 30600) loss: 1.031186\n",
      "(Iteration 2561 / 30600) loss: 1.080206\n",
      "(Iteration 2581 / 30600) loss: 1.352796\n",
      "(Iteration 2601 / 30600) loss: 0.921713\n",
      "(Iteration 2621 / 30600) loss: 0.813918\n",
      "(Iteration 2641 / 30600) loss: 0.953361\n",
      "(Iteration 2661 / 30600) loss: 1.217292\n",
      "(Iteration 2681 / 30600) loss: 1.338492\n",
      "(Iteration 2701 / 30600) loss: 1.011158\n",
      "(Iteration 2721 / 30600) loss: 0.907471\n",
      "(Iteration 2741 / 30600) loss: 1.120007\n",
      "(Iteration 2761 / 30600) loss: 1.024970\n",
      "(Iteration 2781 / 30600) loss: 0.982325\n",
      "(Iteration 2801 / 30600) loss: 0.974478\n",
      "(Iteration 2821 / 30600) loss: 0.971929\n",
      "(Iteration 2841 / 30600) loss: 1.281015\n",
      "(Iteration 2861 / 30600) loss: 1.083394\n",
      "(Iteration 2881 / 30600) loss: 0.980199\n",
      "(Iteration 2901 / 30600) loss: 0.762362\n",
      "(Iteration 2921 / 30600) loss: 1.021350\n",
      "(Iteration 2941 / 30600) loss: 0.821424\n",
      "(Iteration 2961 / 30600) loss: 0.897484\n",
      "(Iteration 2981 / 30600) loss: 0.932065\n",
      "(Iteration 3001 / 30600) loss: 0.980092\n",
      "(Iteration 3021 / 30600) loss: 1.020645\n",
      "(Iteration 3041 / 30600) loss: 0.991130\n",
      "(Epoch 4 / 40) train acc: 0.716000; val_acc: 0.667000\n",
      "(Iteration 3061 / 30600) loss: 1.063219\n",
      "(Iteration 3081 / 30600) loss: 1.049553\n",
      "(Iteration 3101 / 30600) loss: 0.916001\n",
      "(Iteration 3121 / 30600) loss: 0.830330\n",
      "(Iteration 3141 / 30600) loss: 0.854846\n",
      "(Iteration 3161 / 30600) loss: 0.885292\n",
      "(Iteration 3181 / 30600) loss: 0.874605\n",
      "(Iteration 3201 / 30600) loss: 1.018661\n",
      "(Iteration 3221 / 30600) loss: 1.035004\n",
      "(Iteration 3241 / 30600) loss: 0.856860\n",
      "(Iteration 3261 / 30600) loss: 0.967860\n",
      "(Iteration 3281 / 30600) loss: 0.953460\n",
      "(Iteration 3301 / 30600) loss: 0.655760\n",
      "(Iteration 3321 / 30600) loss: 0.923800\n",
      "(Iteration 3341 / 30600) loss: 0.974340\n",
      "(Iteration 3361 / 30600) loss: 0.839922\n",
      "(Iteration 3381 / 30600) loss: 1.015804\n",
      "(Iteration 3401 / 30600) loss: 1.044460\n",
      "(Iteration 3421 / 30600) loss: 0.949850\n",
      "(Iteration 3441 / 30600) loss: 0.837418\n",
      "(Iteration 3461 / 30600) loss: 0.912667\n",
      "(Iteration 3481 / 30600) loss: 0.899656\n",
      "(Iteration 3501 / 30600) loss: 0.922329\n",
      "(Iteration 3521 / 30600) loss: 1.188502\n",
      "(Iteration 3541 / 30600) loss: 1.196347\n",
      "(Iteration 3561 / 30600) loss: 0.780995\n",
      "(Iteration 3581 / 30600) loss: 1.065191\n",
      "(Iteration 3601 / 30600) loss: 0.897254\n",
      "(Iteration 3621 / 30600) loss: 0.991768\n",
      "(Iteration 3641 / 30600) loss: 0.821586\n",
      "(Iteration 3661 / 30600) loss: 0.892848\n",
      "(Iteration 3681 / 30600) loss: 0.977463\n",
      "(Iteration 3701 / 30600) loss: 0.932348\n",
      "(Iteration 3721 / 30600) loss: 1.094475\n",
      "(Iteration 3741 / 30600) loss: 1.024118\n",
      "(Iteration 3761 / 30600) loss: 1.104931\n",
      "(Iteration 3781 / 30600) loss: 0.788745\n",
      "(Iteration 3801 / 30600) loss: 1.068013\n",
      "(Iteration 3821 / 30600) loss: 0.782263\n",
      "(Epoch 5 / 40) train acc: 0.736000; val_acc: 0.658000\n",
      "(Iteration 3841 / 30600) loss: 0.735693\n",
      "(Iteration 3861 / 30600) loss: 0.848227\n",
      "(Iteration 3881 / 30600) loss: 0.955048\n",
      "(Iteration 3901 / 30600) loss: 0.779785\n",
      "(Iteration 3921 / 30600) loss: 0.824318\n",
      "(Iteration 3941 / 30600) loss: 1.031823\n",
      "(Iteration 3961 / 30600) loss: 1.032167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 3981 / 30600) loss: 0.797304\n",
      "(Iteration 4001 / 30600) loss: 0.767465\n",
      "(Iteration 4021 / 30600) loss: 0.726582\n",
      "(Iteration 4041 / 30600) loss: 0.817548\n",
      "(Iteration 4061 / 30600) loss: 0.876040\n",
      "(Iteration 4081 / 30600) loss: 0.712994\n",
      "(Iteration 4101 / 30600) loss: 0.893814\n",
      "(Iteration 4121 / 30600) loss: 0.817756\n",
      "(Iteration 4141 / 30600) loss: 0.873764\n",
      "(Iteration 4161 / 30600) loss: 0.787226\n",
      "(Iteration 4181 / 30600) loss: 0.797102\n",
      "(Iteration 4201 / 30600) loss: 0.999810\n",
      "(Iteration 4221 / 30600) loss: 0.930161\n",
      "(Iteration 4241 / 30600) loss: 0.746312\n",
      "(Iteration 4261 / 30600) loss: 0.965027\n",
      "(Iteration 4281 / 30600) loss: 0.816327\n",
      "(Iteration 4301 / 30600) loss: 0.770595\n",
      "(Iteration 4321 / 30600) loss: 0.875068\n",
      "(Iteration 4341 / 30600) loss: 0.893971\n",
      "(Iteration 4361 / 30600) loss: 0.907313\n",
      "(Iteration 4381 / 30600) loss: 0.936316\n",
      "(Iteration 4401 / 30600) loss: 0.820322\n",
      "(Iteration 4421 / 30600) loss: 0.765956\n",
      "(Iteration 4441 / 30600) loss: 0.857839\n",
      "(Iteration 4461 / 30600) loss: 0.756242\n",
      "(Iteration 4481 / 30600) loss: 0.617578\n",
      "(Iteration 4501 / 30600) loss: 1.002536\n",
      "(Iteration 4521 / 30600) loss: 0.959941\n",
      "(Iteration 4541 / 30600) loss: 0.880624\n",
      "(Iteration 4561 / 30600) loss: 0.830424\n",
      "(Iteration 4581 / 30600) loss: 0.728678\n",
      "(Epoch 6 / 40) train acc: 0.779000; val_acc: 0.696000\n",
      "(Iteration 4601 / 30600) loss: 0.924226\n",
      "(Iteration 4621 / 30600) loss: 0.889222\n",
      "(Iteration 4641 / 30600) loss: 0.741036\n",
      "(Iteration 4661 / 30600) loss: 0.783300\n",
      "(Iteration 4681 / 30600) loss: 0.883142\n",
      "(Iteration 4701 / 30600) loss: 0.705767\n",
      "(Iteration 4721 / 30600) loss: 0.837517\n",
      "(Iteration 4741 / 30600) loss: 0.981823\n",
      "(Iteration 4761 / 30600) loss: 0.701489\n",
      "(Iteration 4781 / 30600) loss: 0.845620\n",
      "(Iteration 4801 / 30600) loss: 0.730134\n",
      "(Iteration 4821 / 30600) loss: 0.901094\n",
      "(Iteration 4841 / 30600) loss: 0.784937\n",
      "(Iteration 4861 / 30600) loss: 0.633407\n",
      "(Iteration 4881 / 30600) loss: 0.824931\n",
      "(Iteration 4901 / 30600) loss: 0.731386\n",
      "(Iteration 4921 / 30600) loss: 0.952635\n",
      "(Iteration 4941 / 30600) loss: 0.715503\n",
      "(Iteration 4961 / 30600) loss: 0.702023\n",
      "(Iteration 4981 / 30600) loss: 0.665536\n",
      "(Iteration 5001 / 30600) loss: 0.679173\n",
      "(Iteration 5021 / 30600) loss: 0.809624\n",
      "(Iteration 5041 / 30600) loss: 0.716361\n",
      "(Iteration 5061 / 30600) loss: 0.814376\n",
      "(Iteration 5081 / 30600) loss: 0.594650\n",
      "(Iteration 5101 / 30600) loss: 0.820913\n",
      "(Iteration 5121 / 30600) loss: 0.759295\n",
      "(Iteration 5141 / 30600) loss: 0.704104\n",
      "(Iteration 5161 / 30600) loss: 0.922154\n",
      "(Iteration 5181 / 30600) loss: 0.721566\n",
      "(Iteration 5201 / 30600) loss: 0.635269\n",
      "(Iteration 5221 / 30600) loss: 0.814261\n",
      "(Iteration 5241 / 30600) loss: 0.956151\n",
      "(Iteration 5261 / 30600) loss: 0.986010\n",
      "(Iteration 5281 / 30600) loss: 0.975541\n",
      "(Iteration 5301 / 30600) loss: 0.697436\n",
      "(Iteration 5321 / 30600) loss: 0.816667\n",
      "(Iteration 5341 / 30600) loss: 0.918353\n",
      "(Epoch 7 / 40) train acc: 0.772000; val_acc: 0.691000\n",
      "(Iteration 5361 / 30600) loss: 0.772317\n",
      "(Iteration 5381 / 30600) loss: 0.707406\n",
      "(Iteration 5401 / 30600) loss: 0.843340\n",
      "(Iteration 5421 / 30600) loss: 0.822524\n",
      "(Iteration 5441 / 30600) loss: 0.844316\n",
      "(Iteration 5461 / 30600) loss: 0.730502\n",
      "(Iteration 5481 / 30600) loss: 0.734749\n",
      "(Iteration 5501 / 30600) loss: 0.741329\n",
      "(Iteration 5521 / 30600) loss: 0.780901\n",
      "(Iteration 5541 / 30600) loss: 0.805582\n",
      "(Iteration 5561 / 30600) loss: 0.742400\n",
      "(Iteration 5581 / 30600) loss: 0.796300\n",
      "(Iteration 5601 / 30600) loss: 0.795214\n",
      "(Iteration 5621 / 30600) loss: 0.760104\n",
      "(Iteration 5641 / 30600) loss: 0.655636\n",
      "(Iteration 5661 / 30600) loss: 0.867827\n",
      "(Iteration 5681 / 30600) loss: 0.668164\n",
      "(Iteration 5701 / 30600) loss: 0.814655\n",
      "(Iteration 5721 / 30600) loss: 0.704398\n",
      "(Iteration 5741 / 30600) loss: 0.789218\n",
      "(Iteration 5761 / 30600) loss: 0.654751\n",
      "(Iteration 5781 / 30600) loss: 0.747730\n",
      "(Iteration 5801 / 30600) loss: 0.708144\n",
      "(Iteration 5821 / 30600) loss: 0.675896\n",
      "(Iteration 5841 / 30600) loss: 0.884723\n",
      "(Iteration 5861 / 30600) loss: 0.635801\n",
      "(Iteration 5881 / 30600) loss: 0.971032\n",
      "(Iteration 5901 / 30600) loss: 0.645423\n",
      "(Iteration 5921 / 30600) loss: 0.779045\n",
      "(Iteration 5941 / 30600) loss: 0.822943\n",
      "(Iteration 5961 / 30600) loss: 0.750222\n",
      "(Iteration 5981 / 30600) loss: 0.880917\n",
      "(Iteration 6001 / 30600) loss: 0.657542\n",
      "(Iteration 6021 / 30600) loss: 0.640304\n",
      "(Iteration 6041 / 30600) loss: 0.557266\n",
      "(Iteration 6061 / 30600) loss: 0.581365\n",
      "(Iteration 6081 / 30600) loss: 0.686700\n",
      "(Iteration 6101 / 30600) loss: 0.865093\n",
      "(Epoch 8 / 40) train acc: 0.826000; val_acc: 0.706000\n",
      "(Iteration 6121 / 30600) loss: 0.675954\n",
      "(Iteration 6141 / 30600) loss: 0.723387\n",
      "(Iteration 6161 / 30600) loss: 0.773022\n",
      "(Iteration 6181 / 30600) loss: 0.828792\n",
      "(Iteration 6201 / 30600) loss: 0.614626\n",
      "(Iteration 6221 / 30600) loss: 0.678552\n",
      "(Iteration 6241 / 30600) loss: 0.682480\n",
      "(Iteration 6261 / 30600) loss: 0.720046\n",
      "(Iteration 6281 / 30600) loss: 0.703159\n",
      "(Iteration 6301 / 30600) loss: 0.666894\n",
      "(Iteration 6321 / 30600) loss: 0.736485\n",
      "(Iteration 6341 / 30600) loss: 0.833015\n",
      "(Iteration 6361 / 30600) loss: 0.657129\n",
      "(Iteration 6381 / 30600) loss: 0.623994\n",
      "(Iteration 6401 / 30600) loss: 0.679855\n",
      "(Iteration 6421 / 30600) loss: 0.752110\n",
      "(Iteration 6441 / 30600) loss: 0.697381\n",
      "(Iteration 6461 / 30600) loss: 0.617059\n",
      "(Iteration 6481 / 30600) loss: 0.761738\n",
      "(Iteration 6501 / 30600) loss: 0.699045\n",
      "(Iteration 6521 / 30600) loss: 0.718983\n",
      "(Iteration 6541 / 30600) loss: 0.754394\n",
      "(Iteration 6561 / 30600) loss: 0.571627\n",
      "(Iteration 6581 / 30600) loss: 0.634908\n",
      "(Iteration 6601 / 30600) loss: 0.604912\n",
      "(Iteration 6621 / 30600) loss: 0.763521\n",
      "(Iteration 6641 / 30600) loss: 0.544179\n",
      "(Iteration 6661 / 30600) loss: 0.589425\n",
      "(Iteration 6681 / 30600) loss: 0.655541\n",
      "(Iteration 6701 / 30600) loss: 0.622906\n",
      "(Iteration 6721 / 30600) loss: 0.672397\n",
      "(Iteration 6741 / 30600) loss: 0.820140\n",
      "(Iteration 6761 / 30600) loss: 0.614275\n",
      "(Iteration 6781 / 30600) loss: 0.652688\n",
      "(Iteration 6801 / 30600) loss: 0.563970\n",
      "(Iteration 6821 / 30600) loss: 0.578923\n",
      "(Iteration 6841 / 30600) loss: 0.666910\n",
      "(Iteration 6861 / 30600) loss: 0.700634\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# batch normalization and dropout useful. Store your best model in the         #\n",
    "# best_model variable.                                                         #\n",
    "################################################################################\n",
    "model = MyCNN2([512, 256, 256, 128, 128,64], weight_scale=4e-3,reg = 0.0002)\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                print_every = 20,\n",
    "                  num_epochs=40, batch_size=64,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 3e-4\n",
    "                  },\n",
    "                  verbose=True)\n",
    "# num_train = 1000\n",
    "# small_data = {\n",
    "#   'X_train': data['X_train'][:num_train],\n",
    "#   'y_train': data['y_train'][:num_train],\n",
    "#   'X_val': data['X_val'],\n",
    "#   'y_val': data['y_val'],\n",
    "# }\n",
    "\n",
    "\n",
    "# solver = Solver(model, small_data,\n",
    "#                 num_epochs=100, batch_size=50,\n",
    "#                 update_rule='adam',\n",
    "#                 optim_config={\n",
    "#                   'learning_rate': 1e-3,\n",
    "#                 },\n",
    "#                 verbose=True, print_every=1)\n",
    "\n",
    "solver.train()\n",
    "solver.train()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(solver.loss_history)\n",
    "plt.title(\"loss history\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(solver.train_acc_history,label = 'train')\n",
    "plt.plot(solver.val_acc_history,label = 'val')\n",
    "plt.title(\"accuracy history\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "best_model = model\n",
    "##############\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
